{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2eaab3",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "This notebook is meant for all EDA. Feel free to add or change sections.\n",
    "\n",
    "\n",
    "Below is a temprorary list of TODOs that you can extend if you notice something you want to check later, but dont have time to do right now.\n",
    "\n",
    "TODO:\n",
    "- Graph out all time-values to see if there is an obious trend over time.\n",
    "- Check for cyclical trends.\n",
    "    - Do sine- and cos-transformations of hour/weekday/month respectively and look for patterns.\n",
    "- Check for outliers.\n",
    "    - Make scatterplots for the continous values and see what values are suspicous.\n",
    "- If there are missing values, look for patterns that wxplain when data is usually missing.\n",
    "    - Do weekends usually have missing data for example?\n",
    "- Check for [class imbalances](https://www.geeksforgeeks.org/machine-learning/how-to-handle-imbalanced-classes-in-machine-learning/).\n",
    "    - If some features have a much lower amount of data points than others, that leads to class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89543289",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bdd0c5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cce498",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39331ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, folder=\"1_raw\"):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file in a subfolder of the project's 'data' directory.\n",
    "    This version is adjusted to work even if the notebook is run from a subfolder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The name of the file to load, including the extension (e.g., \"data.csv\").\n",
    "    folder : str, optional\n",
    "        The subfolder within 'data' to load from. Defaults to \"1_raw\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Go up one level from the current working directory to find the project root\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "        file_path = PROJECT_ROOT / \"data\" / folder / filename\n",
    "\n",
    "        df = pd.read_csv(file_path, sep=\",\")\n",
    "\n",
    "        print(f\"Data loaded successfully from {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_data(df, filename, folder=\"2_interim\"):\n",
    "    \"\"\"\n",
    "    Save a dataframe to a CSV file in a subfolder of the project's 'data' directory.\n",
    "\n",
    "    This function automatically creates the destination folder if it doesn't exist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to save.\n",
    "    filename : str\n",
    "        The name for the output file, including the extension (e.g., \"processed_orders.csv\").\n",
    "    folder : str, optional\n",
    "        The subfolder within 'data' to save to. Defaults to \"2_interim\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "        save_dir = PROJECT_ROOT / \"data\" / folder\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # The full filename, including extension, is now expected\n",
    "        file_path = save_dir / filename\n",
    "\n",
    "        df.to_csv(file_path, sep=\",\", index=False)\n",
    "\n",
    "        print(f\"Data saved successfully to {file_path} ‚úÖ\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b231d10",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8deac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended files\n",
    "df_em = load_data(\"extended_materials.csv\")\n",
    "df_et = load_data(\"extended_transportation.csv\")\n",
    "\n",
    "# Kernel files\n",
    "df_kpo = load_data(\"kernel_purchase_orders.csv\")\n",
    "df_kr = load_data(\"kernel_receivals.csv\")\n",
    "\n",
    "# Other files\n",
    "df_pm = load_data(\"prediction_mapping.csv\")\n",
    "df_ss = load_data(\"sample_submission.csv\")\n",
    "\n",
    "datasets = {\n",
    "    \"extended_materials\": df_em,\n",
    "    \"extended_transportation\": df_et,\n",
    "    \"kernel_purchase_orders\": df_kpo,\n",
    "    \"kernel_receivals\": df_kr,\n",
    "    \"prediction_mapping\": df_pm,\n",
    "    \"sample_submission\": df_ss,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c261e",
   "metadata": {},
   "source": [
    "## Introductory EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7b303",
   "metadata": {},
   "source": [
    "### Checking .head(n=20) for all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_em.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_em.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_et.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a66705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kpo.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e789ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kr.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55256716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef22ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d78bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full Table ---\n",
    "all_features = sorted(\n",
    "    list(set(feature for df in datasets.values() for feature in df.columns))\n",
    ")\n",
    "presence_data = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    row = {}\n",
    "    for feature in all_features:\n",
    "        if feature in df.columns:\n",
    "            series = df[feature]\n",
    "            total_count = len(series)\n",
    "\n",
    "            if total_count > 0:\n",
    "                non_nan_count = series.count()\n",
    "                fill_grade = (non_nan_count / total_count) * 100\n",
    "                row[feature] = f\"‚úÖ {fill_grade:.1f}%\"\n",
    "            else:\n",
    "                row[feature] = \"‚úÖ 100.0%\"\n",
    "        else:\n",
    "            row[feature] = \"\"\n",
    "    presence_data.append(row)\n",
    "\n",
    "presence_df = pd.DataFrame(presence_data, index=datasets.keys())\n",
    "presence_df_transposed = presence_df.T\n",
    "presence_df_transposed.index.name = \"Feature Name\"\n",
    "presence_df_transposed.columns.name = \"DataFrame Name\"\n",
    "\n",
    "print(\"--- Feature Presence & Fill Grade Across All DataFrames ---\")\n",
    "print(\n",
    "    tabulate(presence_df_transposed, headers=\"keys\", tablefmt=\"grid\", stralign=\"center\")\n",
    ")\n",
    "print(\"\\n\" * 2)  # Add some space between the tables\n",
    "\n",
    "\n",
    "# --- Kernel Table ---\n",
    "\n",
    "# 1. Filter the datasets dictionary\n",
    "kernel_datasets = {\n",
    "    \"kernel_purchase_orders\": datasets[\"kernel_purchase_orders\"],\n",
    "    \"kernel_receivals\": datasets[\"kernel_receivals\"],\n",
    "}\n",
    "\n",
    "# 2. Re-run the table generation logic on the filtered dictionary\n",
    "kernel_features = sorted(\n",
    "    list(set(feature for df in kernel_datasets.values() for feature in df.columns))\n",
    ")\n",
    "kernel_presence_data = []\n",
    "\n",
    "for name, df in kernel_datasets.items():\n",
    "    row = {}\n",
    "    for feature in kernel_features:\n",
    "        if feature in df.columns:\n",
    "            series = df[feature]\n",
    "            total_count = len(series)\n",
    "\n",
    "            if total_count > 0:\n",
    "                non_nan_count = series.count()\n",
    "                fill_grade = (non_nan_count / total_count) * 100\n",
    "                row[feature] = f\"‚úÖ {fill_grade:.1f}%\"\n",
    "            else:\n",
    "                row[feature] = \"‚úÖ 100.0%\"\n",
    "        else:\n",
    "            row[feature] = \"\"\n",
    "    kernel_presence_data.append(row)\n",
    "\n",
    "kernel_presence_df = pd.DataFrame(kernel_presence_data, index=kernel_datasets.keys())\n",
    "kernel_presence_df_transposed = kernel_presence_df.T\n",
    "kernel_presence_df_transposed.index.name = \"Feature Name\"\n",
    "kernel_presence_df_transposed.columns.name = \"DataFrame Name\"\n",
    "\n",
    "\n",
    "print(\"--- Feature Presence & Fill Grade for Kernel DataFrames ---\")\n",
    "print(\n",
    "    tabulate(\n",
    "        kernel_presence_df_transposed,\n",
    "        headers=\"keys\",\n",
    "        tablefmt=\"grid\",\n",
    "        stralign=\"center\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080c22f",
   "metadata": {},
   "source": [
    "## 'Advice' regarding what to do next\n",
    "\n",
    "Based on that table and the project requirements, here is a concrete, step-by-step plan for your EDA and data processing. Think of this as a roadmap for your `1_exploratory_data_analysis.ipynb` and `2_data_processing.ipynb` notebooks.\n",
    "\n",
    "### üó∫Ô∏è Your Action Plan: From Raw Data to a Clean Foundation\n",
    "\n",
    "Your goal is to transform the raw CSV files into a single, clean \"master dataframe\" that can be fed into your feature engineering functions.\n",
    "\n",
    "---\n",
    "### **Part 1: Targeted Exploratory Data Analysis (EDA)**\n",
    "*(`1_exploratory_data_analysis.ipynb`)*\n",
    "\n",
    "Your EDA shouldn't be a random search; it should be a targeted investigation to answer specific questions that will help you build features. Here‚Äôs what you're looking for:\n",
    "\n",
    "#### **1. Understand the Target Variable (`net_weight`)**\n",
    "First, get a feel for the delivery patterns.\n",
    "* **Action:**\n",
    "    1.  Load `kernel_receivals.csv`.\n",
    "    2.  Convert `date_arrival` to a datetime object.\n",
    "    3.  Aggregate the data to get the total daily `net_weight` for each `rm_id`.\n",
    "* **What to look for (and visualize):**\n",
    "    * **Sparsity:** Plot the daily deliveries for a few high-volume `rm_id`s. Do deliveries happen every day, or are they large but infrequent? This is critical. A model needs to handle many days with zero deliveries.\n",
    "    * **Seasonality:** Are there weekly patterns (e.g., fewer deliveries on weekends)? Or monthly/quarterly patterns? A bar chart of total deliveries by month or day of the week can reveal this.\n",
    "    * **Outliers:** Are there any negative `net_weight` values? Or values that are orders of magnitude larger than the rest? These could be data entry errors that need cleaning.\n",
    "\n",
    "\n",
    "#### **2. Analyze the Link Between Orders and Receivals (Most Important)**\n",
    "This is where your most powerful predictive signals are.\n",
    "* **Action:**\n",
    "    1.  Merge `kernel_receivals.csv` and `kernel_purchase_orders.csv`.\n",
    "    2.  Calculate the **Delivery Delay**: `delay = actual_date_arrival - expected_delivery_date`. This is your single most important exploratory variable.\n",
    "* **What to look for (and visualize):**\n",
    "    * **Delay Distribution:** Plot a histogram of the `delay` in days. Is it normally distributed around zero? Is there a long tail of very late deliveries? The shape of this distribution tells you how reliable the `delivery_date` is.\n",
    "    * **Segmented Delays:** Does the average delay change based on other categories? Create boxplots of the delay grouped by:\n",
    "        * `supplier_id` (from `receivals`)\n",
    "        * `transporter_name` (from `extended_transportation`)\n",
    "        * `raw_material_format_type` (from `extended_materials`)\n",
    "        If you find that \"Supplier X\" is consistently 5 days late, you've just discovered a hugely valuable feature.\n",
    "\n",
    "---\n",
    "### **Part 2: Data Cleaning and Processing**\n",
    "*(`2_data_processing.ipynb`)*\n",
    "\n",
    "Based on your EDA findings, you can now build a clean, unified dataset.\n",
    "\n",
    "#### **1. Create the `master_df`**\n",
    "* **Action:**\n",
    "    1.  Start with `kernel_receivals`.\n",
    "    2.  **Left join** `kernel_purchase_orders` using `purchase_order_id` and `purchase_order_item_no` as the composite key.\n",
    "    3.  Optionally, left join `extended_materials` (on `rm_id`) and `extended_transportation` (on your composite key) to bring in useful features like `raw_material_format_type` and `transporter_name`.\n",
    "    4.  **Ignore the sparse columns** from `transportation` (e.g., `wood`, `plastic`, `ice`). They won't be useful.\n",
    "\n",
    "#### **2. Clean the Data**\n",
    "* **Action:**\n",
    "    * **Data Types:** Convert all date columns (`date_arrival`, `delivery_date`, `created_date_time`) to the `pd.to_datetime` format.\n",
    "    * **Missing Values:** Your `net_weight` fill rate is 99.9%. The easiest and safest approach is to simply **drop the rows** where `net_weight` is missing. For `batch_id` (52.8% filled), treat the missing values as a special category (e.g., fill with \"Unknown\" or -1).\n",
    "    * **Outliers:** Based on your EDA, remove any obvious errors (e.g., negative `net_weight`).\n",
    "    * **Categorical Features:** For columns like `supplier_id`, `transporter_name`, etc., convert them to pandas' `category` dtype. This is more memory-efficient and LightGBM can handle it directly.\n",
    "\n",
    "#### **3. Save the Processed Data**\n",
    "* **Action:** Save your clean `master_df` to the `data/3_processed` directory as a Parquet or Feather file. This is much faster to load than a CSV and preserves your data types, so you don't have to repeat the cleaning steps every time you work on your model.\n",
    "\n",
    "Once you have this clean `master_df`, you are perfectly set up to start implementing the `create_features` and `create_target` functions in your `lgbm_pipeline.py` script. Your data work will then directly feed your modeling pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4173-course-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
