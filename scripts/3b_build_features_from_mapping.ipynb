{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbb6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TIMJ\\AppData\\Local\\Temp\\ipykernel_2544\\277479936.py:66: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(complete_calendar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 30450 rows -> c:\\Users\\TIMJ\\tdt4173-course-project\\data\\3_processed\\features_for_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Stier\n",
    "# ------------------------------------------------------------\n",
    "DATA = Path.cwd() / \"data\"\n",
    "if not DATA.exists():\n",
    "    DATA = Path.cwd().parent / \"data\"\n",
    "\n",
    "RAW        = DATA / \"1_raw\"\n",
    "INTERIM    = DATA / \"2_interim\"\n",
    "PROCESSED  = DATA / \"3_processed\"\n",
    "INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PREDICTION_MAPPING_CSV = RAW / \"prediction_mapping.csv\"\n",
    "MASTER_DATA_CSV        = INTERIM / \"master_data.csv\"\n",
    "PURCHASE_ORDERS_CSV    = RAW / \"kernel_purchase_orders.csv\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Hjelpefunksjoner\n",
    "# ------------------------------------------------------------\n",
    "def _prep_asof_keys(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"rm_id\" in out.columns:\n",
    "        out[\"rm_id\"] = pd.to_numeric(out[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    if \"date\" in out.columns:\n",
    "        out[\"date\"] = pd.to_datetime(out[\"date\"], utc=False, errors=\"coerce\")\n",
    "    out = out.dropna(subset=[c for c in [\"rm_id\",\"date\"] if c in out.columns])\n",
    "    out = out.sort_values([\"rm_id\",\"date\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def read_mapping(mapping_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(mapping_path, parse_dates=[\"forecast_start_date\", \"forecast_end_date\"])\n",
    "    return df[[\"ID\",\"rm_id\",\"forecast_start_date\",\"forecast_end_date\"]].copy()\n",
    "\n",
    "def read_master(master_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(master_path, low_memory=False)\n",
    "    df[\"date_arrival\"] = pd.to_datetime(df[\"date_arrival\"], utc=True, errors=\"coerce\").dt.tz_convert(None)\n",
    "    df[\"net_weight\"]   = pd.to_numeric(df.get(\"net_weight\"), errors=\"coerce\").fillna(0.0)\n",
    "    for c in [\"rm_id\",\"purchase_order_id\",\"purchase_order_item_no\",\"receival_item_no\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "    recv_key = [\"rm_id\",\"purchase_order_id\",\"purchase_order_item_no\",\"receival_item_no\"]\n",
    "    if not set(recv_key).issubset(df.columns):\n",
    "        recv_key = [\"rm_id\",\"date_arrival\",\"net_weight\"]\n",
    "    df = df.drop_duplicates(subset=recv_key, keep=\"first\").copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date_arrival\"].dt.date)\n",
    "    return df\n",
    "\n",
    "def complete_calendar(daily_rm: pd.DataFrame) -> pd.DataFrame:\n",
    "    full = pd.DataFrame({\"date\": pd.date_range(daily_rm[\"date\"].min(), daily_rm[\"date\"].max(), freq=\"D\")})\n",
    "    full[\"rm_id\"] = daily_rm.name\n",
    "    out = full.merge(daily_rm, on=[\"rm_id\",\"date\"], how=\"left\")\n",
    "    out[\"net_weight\"] = out[\"net_weight\"].fillna(0.0)\n",
    "    return out\n",
    "\n",
    "def build_daily(master_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    master_df = master_df.copy()\n",
    "    master_df[\"rm_id\"] = pd.to_numeric(master_df[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    daily = (master_df.groupby([\"rm_id\",\"date\"], as_index=False)[\"net_weight\"].sum()\n",
    "             .sort_values([\"rm_id\",\"date\"]))\n",
    "    daily = (daily.groupby(\"rm_id\", group_keys=False)\n",
    "                   .apply(complete_calendar)\n",
    "                   .sort_values([\"rm_id\",\"date\"])\n",
    "                   .reset_index(drop=True))\n",
    "    daily[\"cum_kg\"] = daily.groupby(\"rm_id\")[\"net_weight\"].cumsum()\n",
    "    daily[\"deliveries_flag\"] = (daily[\"net_weight\"] > 0).astype(int)\n",
    "    for w in [7,14,28,56,365]:\n",
    "        daily[f\"r{w}_kg\"]   = daily.groupby(\"rm_id\")[\"net_weight\"].rolling(w, min_periods=1).sum().reset_index(level=0, drop=True)\n",
    "        daily[f\"r{w}_days\"] = daily.groupby(\"rm_id\")[\"deliveries_flag\"].rolling(w, min_periods=1).sum().reset_index(level=0, drop=True)\n",
    "    def safe_div(a, b):\n",
    "        b = b.replace(0, np.nan)\n",
    "        return (a / b).fillna(0.0)\n",
    "    daily[\"r28_mean_kg_per_day\"] = safe_div(daily[\"r28_kg\"], daily[\"r28_days\"])\n",
    "    daily[\"r56_mean_kg_per_day\"] = safe_div(daily[\"r56_kg\"], daily[\"r56_days\"])\n",
    "    tmp = daily[[\"rm_id\",\"date\",\"net_weight\"]].copy()\n",
    "    tmp[\"last_deliv_date\"] = tmp[\"date\"].where(tmp[\"net_weight\"] > 0)\n",
    "    tmp[\"last_deliv_date\"] = tmp.groupby(\"rm_id\")[\"last_deliv_date\"].ffill()\n",
    "    daily[\"days_since_last\"] = (daily[\"date\"] - tmp[\"last_deliv_date\"]).dt.days.fillna(10_000).astype(int)\n",
    "    return daily\n",
    "\n",
    "def read_orders_with_rm(orders_path: Path | None, master_df: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    if orders_path is None or not orders_path.exists():\n",
    "        return None\n",
    "    po = pd.read_csv(\n",
    "        orders_path,\n",
    "        low_memory=False,\n",
    "        parse_dates=[\"delivery_date\",\"created_date_time\",\"modified_date_time\"]\n",
    "    )\n",
    "    for c in [\"purchase_order_id\",\"purchase_order_item_no\"]:\n",
    "        if c in po.columns:\n",
    "            po[c] = pd.to_numeric(po[c], errors=\"coerce\").astype(\"Int64\")\n",
    "    po[\"quantity\"] = pd.to_numeric(po.get(\"quantity\"), errors=\"coerce\").fillna(0.0)\n",
    "    rm_lookup = (master_df[[\"rm_id\",\"purchase_order_id\",\"purchase_order_item_no\"]]\n",
    "                 .dropna()\n",
    "                 .drop_duplicates())\n",
    "    po = po.merge(rm_lookup, on=[\"purchase_order_id\",\"purchase_order_item_no\"], how=\"left\")\n",
    "    if \"delivery_date\" in po.columns:\n",
    "        po[\"delivery_date\"] = pd.to_datetime(po[\"delivery_date\"], utc=True, errors=\"coerce\").dt.tz_convert(None)\n",
    "    if \"created_date_time\" in po.columns:\n",
    "        po[\"created_date_time\"] = pd.to_datetime(po[\"created_date_time\"], utc=True, errors=\"coerce\").dt.tz_convert(None)\n",
    "    if \"modified_date_time\" in po.columns:\n",
    "        po[\"modified_date_time\"] = pd.to_datetime(po[\"modified_date_time\"], utc=True, errors=\"coerce\").dt.tz_convert(None)\n",
    "    keep_cols = [c for c in [\"rm_id\",\"quantity\",\"delivery_date\",\"created_date_time\",\"modified_date_time\",\"purchase_order_id\",\"purchase_order_item_no\"] if c in po.columns]\n",
    "    po = po[keep_cols].dropna(subset=[\"rm_id\",\"delivery_date\"]).copy()\n",
    "    po[\"rm_id\"] = pd.to_numeric(po[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    return po\n",
    "\n",
    "def same_period_last_year(df_map: pd.DataFrame, csum: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Beregner total vekt i samme periode i fjor.\n",
    "    Bruker groupby og apply i stedet for merge_asof for å unngå sorteringsproblemer.\n",
    "    \"\"\"\n",
    "    df_map = df_map.copy()\n",
    "    df_map[\"rm_id\"] = pd.to_numeric(df_map[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    \n",
    "    # Forbered cumulative data\n",
    "    csum_clean = csum[[\"rm_id\",\"date\",\"cum_kg\"]].copy()\n",
    "    csum_clean[\"rm_id\"] = pd.to_numeric(csum_clean[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    csum_clean[\"date\"] = pd.to_datetime(csum_clean[\"date\"], errors=\"coerce\")\n",
    "    csum_clean = csum_clean.dropna(subset=[\"rm_id\",\"date\"])\n",
    "    csum_clean = csum_clean.sort_values([\"rm_id\",\"date\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Beregn datoer for i fjor\n",
    "    df_map[\"prev_start\"] = df_map[\"forecast_start_date\"] - pd.Timedelta(days=366)\n",
    "    df_map[\"prev_end\"] = df_map[\"forecast_end_date\"] - pd.Timedelta(days=365)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Gruppe etter rm_id og beregn for hver gruppe\n",
    "    for rm_id, group in df_map.groupby(\"rm_id\"):\n",
    "        # Hent cumulative data for denne rm_id\n",
    "        rm_csum = csum_clean[csum_clean[\"rm_id\"] == rm_id].copy()\n",
    "        \n",
    "        if rm_csum.empty:\n",
    "            # Ingen data for denne rm_id\n",
    "            for idx, row in group.iterrows():\n",
    "                results.append({\"ID\": row[\"ID\"], \"same_period_last_year_kg\": 0.0})\n",
    "            continue\n",
    "        \n",
    "        # For hver rad i gruppen\n",
    "        for idx, row in group.iterrows():\n",
    "            prev_start = row[\"prev_start\"]\n",
    "            prev_end = row[\"prev_end\"]\n",
    "            \n",
    "            # Finn nærmeste dato <= prev_start\n",
    "            before_start = rm_csum[rm_csum[\"date\"] <= prev_start]\n",
    "            cum_a = before_start[\"cum_kg\"].iloc[-1] if not before_start.empty else 0.0\n",
    "            \n",
    "            # Finn nærmeste dato <= prev_end\n",
    "            before_end = rm_csum[rm_csum[\"date\"] <= prev_end]\n",
    "            cum_b = before_end[\"cum_kg\"].iloc[-1] if not before_end.empty else 0.0\n",
    "            \n",
    "            # Beregn differanse\n",
    "            diff = max(0.0, cum_b - cum_a)\n",
    "            results.append({\"ID\": row[\"ID\"], \"same_period_last_year_kg\": diff})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def add_orders_features(df_map: pd.DataFrame, orders: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    if orders is None or orders.empty:\n",
    "        df_map[\"orders_qty_in_window\"] = 0.0\n",
    "        df_map[\"orders_lines_in_window\"] = 0\n",
    "        return df_map\n",
    "    ordr = df_map[[\"ID\",\"rm_id\",\"forecast_start_date\",\"forecast_end_date\",\"cutoff_date\"]].merge(orders, on=\"rm_id\", how=\"left\")\n",
    "    known_mask = True\n",
    "    if \"created_date_time\" in ordr.columns:\n",
    "        known_mask = ordr[\"created_date_time\"] <= ordr[\"cutoff_date\"]\n",
    "    if \"modified_date_time\" in ordr.columns:\n",
    "        known_mask &= (ordr[\"modified_date_time\"].isna() | (ordr[\"modified_date_time\"] <= ordr[\"cutoff_date\"]))\n",
    "    ordr = ordr[known_mask]\n",
    "    in_win = (ordr[\"delivery_date\"] >= ordr[\"forecast_start_date\"]) & (ordr[\"delivery_date\"] <= ordr[\"forecast_end_date\"])\n",
    "    if {\"purchase_order_id\",\"purchase_order_item_no\"}.issubset(ordr.columns):\n",
    "        grp = ordr[in_win].groupby(\"ID\", as_index=False).agg(\n",
    "            orders_qty_in_window=(\"quantity\",\"sum\"),\n",
    "            orders_lines_in_window=(\"purchase_order_id\",\"nunique\")\n",
    "        )\n",
    "    else:\n",
    "        grp = ordr[in_win].groupby(\"ID\", as_index=False).agg(\n",
    "            orders_qty_in_window=(\"quantity\",\"sum\"),\n",
    "            orders_lines_in_window=(\"quantity\",\"size\")\n",
    "        )\n",
    "    out = df_map.merge(grp, on=\"ID\", how=\"left\")\n",
    "    out[\"orders_qty_in_window\"] = out[\"orders_qty_in_window\"].fillna(0.0)\n",
    "    out[\"orders_lines_in_window\"] = out[\"orders_lines_in_window\"].fillna(0).astype(int)\n",
    "    return out\n",
    "\n",
    "def make_features_from_mapping(daily: pd.DataFrame, df_map_in: pd.DataFrame, orders: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    df_map = df_map_in.copy()\n",
    "    df_map[\"rm_id\"] = pd.to_numeric(df_map[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    df_map[\"cutoff_date\"] = df_map[\"forecast_start_date\"] - pd.Timedelta(days=1)\n",
    "    df_map[\"window_days\"] = (df_map[\"forecast_end_date\"] - df_map[\"forecast_start_date\"]).dt.days + 1\n",
    "    feat_date = daily.rename(columns={\"date\":\"cutoff_date\"}).copy()\n",
    "    feat_date[\"rm_id\"] = pd.to_numeric(feat_date[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    feat_date = feat_date.sort_values([\"rm_id\",\"cutoff_date\"])\n",
    "    hist_cols = [\"rm_id\",\"cutoff_date\",\"cum_kg\",\"r7_kg\",\"r14_kg\",\"r28_kg\",\"r56_kg\",\"r365_kg\",\n",
    "                 \"r7_days\",\"r14_days\",\"r28_days\",\"r56_days\",\"r365_days\",\n",
    "                 \"r28_mean_kg_per_day\",\"r56_mean_kg_per_day\",\"days_since_last\"]\n",
    "    X = df_map.merge(feat_date[hist_cols], on=[\"rm_id\",\"cutoff_date\"], how=\"left\")\n",
    "    csum = daily[[\"rm_id\",\"date\",\"cum_kg\"]].copy()\n",
    "    X = X.merge(same_period_last_year(df_map, csum), on=\"ID\", how=\"left\")\n",
    "    X[\"start_month\"] = X[\"forecast_start_date\"].dt.month\n",
    "    X[\"start_dow\"]   = X[\"forecast_start_date\"].dt.dayofweek\n",
    "    X[\"end_month\"]   = X[\"forecast_end_date\"].dt.month\n",
    "    X = add_orders_features(X, orders)\n",
    "    for c in [\"cum_kg\",\"r7_kg\",\"r14_kg\",\"r28_kg\",\"r56_kg\",\"r365_kg\",\n",
    "              \"r7_days\",\"r14_days\",\"r28_days\",\"r56_days\",\"r365_days\",\n",
    "              \"r28_mean_kg_per_day\",\"r56_mean_kg_per_day\",\"days_since_last\",\n",
    "              \"same_period_last_year_kg\",\"orders_qty_in_window\"]:\n",
    "        if c in X.columns:\n",
    "            X[c] = X[c].fillna(0.0)\n",
    "    return X\n",
    "\n",
    "def build_training_windows_template(daily: pd.DataFrame, mapping_like: pd.DataFrame,\n",
    "                                    step_days: int = 7, min_history_days: int = 400) -> pd.DataFrame:\n",
    "    win_lengths = sorted(((mapping_like[\"forecast_end_date\"] - mapping_like[\"forecast_start_date\"]).dt.days + 1).unique())\n",
    "    pieces = []\n",
    "    for rm, g in daily.groupby(\"rm_id\"):\n",
    "        first_date = g[\"date\"].min()\n",
    "        last_date  = g[\"date\"].max()\n",
    "        for wlen in win_lengths:\n",
    "            start = first_date + pd.Timedelta(days=min_history_days)\n",
    "            end_latest = last_date - pd.Timedelta(days=wlen)\n",
    "            if start > end_latest:\n",
    "                continue\n",
    "            starts = pd.date_range(start, end_latest, freq=f\"{step_days}D\")\n",
    "            df = pd.DataFrame({\n",
    "                \"rm_id\": rm,\n",
    "                \"forecast_start_date\": starts,\n",
    "                \"forecast_end_date\": starts + pd.to_timedelta(wlen - 1, unit=\"D\")\n",
    "            })\n",
    "            pieces.append(df)\n",
    "    if not pieces:\n",
    "        return pd.DataFrame(columns=[\"ID\",\"rm_id\",\"forecast_start_date\",\"forecast_end_date\"])\n",
    "    tmpl = pd.concat(pieces, ignore_index=True)\n",
    "    tmpl = tmpl.sort_values([\"rm_id\",\"forecast_start_date\"]).reset_index(drop=True)\n",
    "    tmpl[\"ID\"] = np.arange(1, len(tmpl) + 1)\n",
    "    return tmpl[[\"ID\",\"rm_id\",\"forecast_start_date\",\"forecast_end_date\"]]\n",
    "\n",
    "def label_from_daily(daily: pd.DataFrame, df_map: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = daily[[\"rm_id\",\"date\",\"net_weight\"]].copy()\n",
    "    g = _prep_asof_keys(g)\n",
    "    g[\"cum\"] = g.groupby(\"rm_id\")[\"net_weight\"].cumsum()\n",
    "    a = df_map[[\"ID\",\"rm_id\",\"forecast_start_date\"]].rename(columns={\"forecast_start_date\":\"date\"})\n",
    "    b = df_map[[\"ID\",\"rm_id\",\"forecast_end_date\"]].rename(columns={\"forecast_end_date\":\"date\"})\n",
    "    a = _prep_asof_keys(a)\n",
    "    b = _prep_asof_keys(b)\n",
    "    a = pd.merge_asof(a, g[[\"rm_id\",\"date\",\"cum\"]], by=\"rm_id\", on=\"date\", direction=\"backward\")\n",
    "    b = pd.merge_asof(b, g[[\"rm_id\",\"date\",\"cum\"]], by=\"rm_id\", on=\"date\", direction=\"backward\")\n",
    "    y = (b[[\"ID\",\"cum\"]].rename(columns={\"cum\":\"cum_b\"})\n",
    "         .merge(a[[\"ID\",\"cum\"]].rename(columns={\"cum\":\"cum_a\"}), on=\"ID\", how=\"left\"))\n",
    "    y[\"y_window_kg\"] = (y[\"cum_b\"] - y[\"cum_a\"]).fillna(0.0)\n",
    "    return y[[\"ID\",\"y_window_kg\"]]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Kjøring\n",
    "# ------------------------------------------------------------\n",
    "df_map_raw = read_mapping(PREDICTION_MAPPING_CSV)\n",
    "df_master  = read_master(MASTER_DATA_CSV)\n",
    "daily      = build_daily(df_master)\n",
    "orders     = read_orders_with_rm(PURCHASE_ORDERS_CSV, df_master)\n",
    "\n",
    "df_map_raw.to_csv(PROCESSED / \"prediction_mapping_base_copy.csv\", index=False)\n",
    "\n",
    "X_submit = make_features_from_mapping(daily, df_map_raw, orders)\n",
    "\n",
    "out_cols = [\"ID\",\"rm_id\",\"forecast_start_date\",\"forecast_end_date\",\"cutoff_date\",\"window_days\",\n",
    "            \"cum_kg\",\"r7_kg\",\"r14_kg\",\"r28_kg\",\"r56_kg\",\"r365_kg\",\n",
    "            \"r7_days\",\"r14_days\",\"r28_days\",\"r56_days\",\"r365_days\",\n",
    "            \"r28_mean_kg_per_day\",\"r56_mean_kg_per_day\",\"days_since_last\",\n",
    "            \"same_period_last_year_kg\",\n",
    "            \"orders_qty_in_window\",\"orders_lines_in_window\",\n",
    "            \"start_month\",\"start_dow\",\"end_month\"]\n",
    "X_submit[out_cols].to_csv(PROCESSED / \"features_for_submission.csv\", index=False)\n",
    "print(f\"Wrote {len(X_submit)} rows -> {PROCESSED/'features_for_submission.csv'}\")\n",
    "\n",
    "try:\n",
    "    train_template = build_training_windows_template(daily, df_map_raw, step_days=7, min_history_days=400)\n",
    "    X_train = make_features_from_mapping(daily, train_template, orders)\n",
    "    y_train = label_from_daily(daily, train_template)\n",
    "    train = X_train.merge(y_train, on=\"ID\", how=\"left\")\n",
    "    train[out_cols + [\"y_window_kg\"]].to_csv(PROCESSED / \"training_features_and_labels.csv\", index=False)\n",
    "    print(f\"Wrote {len(train)} rows -> {PROCESSED/'training_features_and_labels.csv'}\")\n",
    "except Exception as e:\n",
    "    print(\"Training set build skipped or failed:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4173-course-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
