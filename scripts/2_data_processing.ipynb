{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b92373",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "This notebook is meant for all data cleaning and processing. Feel free to add or change sections.\n",
    "\n",
    "Some Todos:\n",
    "- `kernel_receivals`\n",
    "    - Remove all rows where `rm_id`'s don't exist in `prediction_mapping`.\n",
    "    - Remove all rows in `kernel_purchase_orders` with `quantity  <= 0`.\n",
    "    - Remove `product_id` and `receival_status`.\n",
    "    - Remove `batch_id`. It appears only from 2015 and onwards --> Poor data quality.\n",
    "- `kernel_purchase_orders`\n",
    "    - Remove `product_id`. There are multiple `rm_id`'s for each product_id, therefore it is useless here. We will know the rm_id after the dataset is joined with kernel_receivals based on purchase_order_id and/or purchase_order_item_no.\n",
    "    - Remove `unit` and `unit_id`. Almost all entries have KG and the ones that do not are simply NaN and can be assumed to be KG as well.\n",
    "    - Remove `status_id`. The status is duplicated in `status`.\n",
    "    - Remove `product_version`. It is just set to 1 for all orders up until 2016 --> Poor data quality.\n",
    "- `extended_materials`\n",
    "    - Remove all rows where `rm_id`'s don't exist in `prediction_mapping`.\n",
    "    - Remove `raw_material_alloy`, `product_id`.\n",
    "    - There are a good deal of entries where `stock_location` is prefixed with DELETED followed by a date. It would be useful to check if there are differences in deliveries before and after that date. My hypothesis is that after it is deleted, there are no more deliveries for that rm_id. If this is a case, a boolean feature `is_deleted` would be powerful. Before inserting a DELETED=TRUE tag for an rm_id, check that there are no other listings of the same rm_id's that do not have DELETED in the stock_location variable. If there are other non-DELETED locations, then it may be that the stock location has simply moved but is still active. Be sure to also make a `deleted_date` feature as well. Then it is possible to create a `days_since_deleted = (forecast_date - deleted_date)` feature.\n",
    "    - Remove `stock_location`, but only after making the `is_deleted` boolean and`date_deleted`.\n",
    "- `extended_transportation`\n",
    "    - Remove all rows where `rm_id`'s don't exist in `prediction_mapping`.\n",
    "    - Remove `product_id`.\n",
    "    - Remove `unit_status`. Almost all entries are 'Tranferred', a some are 'Accepted' and a couple are 'Pending'.\n",
    "    - Remove `wood`, `ironbands`, `plastic`, `water`, `ice`, `other`, `chips`, `packaging`, `cardboard`. These have very few entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d5deb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90550889",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4fec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da89dc5e",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e51765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, folder=\"1_raw\"):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file in a subfolder of the project's 'data' directory.\n",
    "    This version is adjusted to work even if the notebook is run from a subfolder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The name of the file to load, including the extension (e.g., \"data.csv\").\n",
    "    folder : str, optional\n",
    "        The subfolder within 'data' to load from. Defaults to \"1_raw\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Go up one level from the current working directory to find the project root\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "        file_path = PROJECT_ROOT / \"data\" / folder / filename\n",
    "\n",
    "        df = pd.read_csv(file_path, sep=\",\")\n",
    "\n",
    "        print(f\"Data loaded successfully from {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_data(df, filename, folder=\"2_interim\"):\n",
    "    \"\"\"\n",
    "    Save a dataframe to a CSV file in a subfolder of the project's 'data' directory.\n",
    "\n",
    "    This function automatically creates the destination folder if it doesn't exist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to save.\n",
    "    filename : str\n",
    "        The name for the output file, including the extension (e.g., \"processed_orders.csv\").\n",
    "    folder : str, optional\n",
    "        The subfolder within 'data' to save to. Defaults to \"2_interim\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "        save_dir = PROJECT_ROOT / \"data\" / folder\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # The full filename, including extension, is now expected\n",
    "        file_path = save_dir / filename\n",
    "\n",
    "        df.to_csv(file_path, sep=\",\", index=False)\n",
    "\n",
    "        print(f\"Data saved successfully to {file_path} ✅\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the file: {e}\")\n",
    "\n",
    "\n",
    "def generate_feature_presence_table(datasets: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    Analyzes a dictionary of pandas DataFrames to determine the presence and\n",
    "    fill grade of features (columns) across all of them. It then prints a\n",
    "    formatted table summarizing this information.\n",
    "\n",
    "    Args:\n",
    "        datasets (Dict[str, pd.DataFrame]): A dictionary where keys are string\n",
    "            names for the DataFrames and values are the pandas DataFrames\n",
    "            themselves.\n",
    "    \"\"\"\n",
    "    if not isinstance(datasets, dict) or not all(\n",
    "        isinstance(df, pd.DataFrame) for df in datasets.values()\n",
    "    ):\n",
    "        print(\"Error: Input must be a dictionary of pandas DataFrames.\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Identify all unique features across all DataFrames ---\n",
    "    all_features = sorted(\n",
    "        list(set(feature for df in datasets.values() for feature in df.columns))\n",
    "    )\n",
    "\n",
    "    # --- 2. Build the presence and fill grade data for each DataFrame ---\n",
    "    presence_data = []\n",
    "    for name, df in datasets.items():\n",
    "        row = {\"DataFrame Name\": name}\n",
    "        for feature in all_features:\n",
    "            if feature in df.columns:\n",
    "                series = df[feature]\n",
    "                total_count = len(series)\n",
    "\n",
    "                if total_count > 0:\n",
    "                    non_nan_count = series.count()  # .count() excludes NaNs\n",
    "                    fill_grade = (non_nan_count / total_count) * 100\n",
    "                    row[feature] = f\"✅ {fill_grade:.1f}%\"\n",
    "                else:\n",
    "                    # Handle empty DataFrames/Series\n",
    "                    row[feature] = \"✅ 100.0%\"\n",
    "            else:\n",
    "                # Feature is not present in this DataFrame\n",
    "                row[feature] = \"\"\n",
    "        presence_data.append(row)\n",
    "\n",
    "    # --- 3. Create, transpose, and format the DataFrame for display ---\n",
    "    if not presence_data:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "\n",
    "    presence_df = pd.DataFrame(presence_data)\n",
    "    presence_df = presence_df.set_index(\"DataFrame Name\")\n",
    "\n",
    "    # Transpose the DataFrame so features are rows and datasets are columns\n",
    "    presence_df_transposed = presence_df.T\n",
    "    presence_df_transposed.index.name = \"Feature Name\"\n",
    "\n",
    "    # --- 4. Print the final table using tabulate ---\n",
    "    print(\"--- Feature Presence & Fill Grade Across All DataFrames ---\")\n",
    "    print(\n",
    "        tabulate(\n",
    "            presence_df_transposed, headers=\"keys\", tablefmt=\"grid\", stralign=\"center\"\n",
    "        )\n",
    "    )\n",
    "    print(\"\\n\" * 2)  # Add some space after the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febc650",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6963740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /home/kfkh/Code/tdt4173-course-project/data/1_raw/kernel_purchase_orders.csv\n",
      "Data loaded successfully from /home/kfkh/Code/tdt4173-course-project/data/1_raw/kernel_receivals.csv\n",
      "Data loaded successfully from /home/kfkh/Code/tdt4173-course-project/data/1_raw/extended_materials.csv\n",
      "Data loaded successfully from /home/kfkh/Code/tdt4173-course-project/data/1_raw/extended_transportation.csv\n"
     ]
    }
   ],
   "source": [
    "# Kernel files\n",
    "df_kpo = load_data(\"kernel_purchase_orders.csv\")\n",
    "df_kr = load_data(\"kernel_receivals.csv\")\n",
    "\n",
    "# Extended files\n",
    "df_em = load_data(\"extended_materials.csv\")\n",
    "df_et = load_data(\"extended_transportation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6300ad",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd785a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature Presence & Fill Grade Across All DataFrames ---\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|       Feature Name       |  kernel_receivals  |  kernel_purchase_orders  |  extended_materials  |  extended_transportation  |\n",
      "+==========================+====================+==========================+======================+===========================+\n",
      "|         batch_id         |      ✅ 52.8%      |                          |                      |         ✅ 52.8%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|        cardboard         |                    |                          |                      |          ✅ 6.4%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|          chips           |                    |                          |                      |          ✅ 6.4%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|    created_date_time     |                    |        ✅ 100.0%         |                      |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|       date_arrival       |     ✅ 100.0%      |                          |                      |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|      delivery_date       |                    |        ✅ 100.0%         |                      |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|       gross_weight       |                    |                          |                      |         ✅ 99.9%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|           ice            |                    |                          |                      |          ✅ 6.4%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|        ironbands         |                    |                          |                      |          ✅ 9.0%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|    modified_date_time    |                    |         ✅ 98.5%         |                      |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|        net_weight        |      ✅ 99.9%      |                          |                      |         ✅ 99.9%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|          other           |                    |                          |                      |          ✅ 6.8%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|        packaging         |                    |                          |                      |          ✅ 6.4%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|         plastic          |                    |                          |                      |         ✅ 13.1%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|        product_id        |     ✅ 100.0%      |        ✅ 100.0%         |       ✅ 99.9%       |         ✅ 100.0%         |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|     product_version      |                    |        ✅ 100.0%         |       ✅ 99.9%       |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|    purchase_order_id     |     ✅ 100.0%      |        ✅ 100.0%         |                      |         ✅ 100.0%         |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|  purchase_order_item_no  |     ✅ 100.0%      |        ✅ 100.0%         |                      |         ✅ 100.0%         |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|         quantity         |                    |        ✅ 100.0%         |                      |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|    raw_material_alloy    |                    |                          |       ✅ 99.9%       |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "| raw_material_format_type |                    |                          |       ✅ 99.9%       |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|     receival_item_no     |     ✅ 100.0%      |                          |                      |         ✅ 100.0%         |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|     receival_status      |     ✅ 100.0%      |                          |                      |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|          rm_id           |     ✅ 100.0%      |                          |       ✅ 99.9%       |         ✅ 100.0%         |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|          status          |                    |        ✅ 100.0%         |                      |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|        status_id         |                    |        ✅ 100.0%         |                      |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|      stock_location      |                    |                          |       ✅ 99.9%       |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|       supplier_id        |     ✅ 100.0%      |                          |                      |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|       tare_weight        |                    |                          |                      |         ✅ 88.1%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|     transporter_name     |                    |                          |                      |         ✅ 100.0%         |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|           unit           |                    |         ✅ 99.9%         |                      |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|         unit_id          |                    |         ✅ 99.9%         |                      |                           |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|       unit_status        |                    |                          |                      |         ✅ 100.0%         |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|    vehicle_end_weight    |                    |                          |                      |         ✅ 100.0%         |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|        vehicle_no        |                    |                          |                      |         ✅ 100.0%         |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|   vehicle_start_weight   |                    |                          |                      |         ✅ 100.0%         |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|          water           |                    |                          |                      |          ✅ 6.4%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "|           wood           |                    |                          |                      |         ✅ 20.6%          |\n",
      "+--------------------------+--------------------+--------------------------+----------------------+---------------------------+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Order datasets for iteration\n",
    "datasets = {\n",
    "    \"kernel_receivals\": df_kr,\n",
    "    \"kernel_purchase_orders\": df_kpo,\n",
    "    \"extended_materials\": df_em,\n",
    "    \"extended_transportation\": df_et,\n",
    "}\n",
    "generate_feature_presence_table(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba9eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /home/kfkh/Code/tdt4173-course-project/data/1_raw/prediction_mapping.csv\n",
      "Found 203 unique rm_id's to focus on.\n"
     ]
    }
   ],
   "source": [
    "# Load the mapping file to get the relevant rm_id's\n",
    "df_map = load_data(\"prediction_mapping.csv\")\n",
    "relevant_rm_ids = df_map[\"rm_id\"].unique()\n",
    "print(f\"Found {len(relevant_rm_ids)} unique rm_id's to focus on.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32bb5dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing kernel_receivals ---\n",
      "Filtered rm_ids: Kept 122533 rows out of 122590.\n",
      "Removed columns: product_id, receival_status, batch_id.\n",
      "kernel_receivals processing complete.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 122533 entries, 0 to 122589\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   rm_id                   122533 non-null  float64\n",
      " 1   purchase_order_id       122533 non-null  float64\n",
      " 2   purchase_order_item_no  122533 non-null  float64\n",
      " 3   receival_item_no        122533 non-null  int64  \n",
      " 4   date_arrival            122533 non-null  object \n",
      " 5   net_weight              122520 non-null  float64\n",
      " 6   supplier_id             122533 non-null  int64  \n",
      "dtypes: float64(4), int64(2), object(1)\n",
      "memory usage: 7.5+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4245/2481872611.py:16: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_kr_processed['date_arrival'] = pd.to_datetime(df_kr_processed['date_arrival'])\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Processing kernel_receivals ---\")\n",
    "\n",
    "# Start with a copy to avoid modifying the original dataframe\n",
    "df_kr_processed = df_kr.copy()\n",
    "\n",
    "# Filter for relevant rm_id's\n",
    "initial_rows = len(df_kr_processed)\n",
    "df_kr_processed = df_kr_processed[df_kr_processed[\"rm_id\"].isin(relevant_rm_ids)]\n",
    "print(f\"Filtered rm_ids: Kept {len(df_kr_processed)} rows out of {initial_rows}.\")\n",
    "\n",
    "# Remove specified columns\n",
    "df_kr_processed = df_kr_processed.drop(\n",
    "    columns=[\"product_id\", \"receival_status\", \"batch_id\"]\n",
    ")\n",
    "print(\"Removed columns: product_id, receival_status, batch_id.\")\n",
    "\n",
    "# Ensure date_arrival is a datetime object for future joins and feature engineering\n",
    "df_kr_processed[\"date_arrival\"] = pd.to_datetime(df_kr_processed[\"date_arrival\"])\n",
    "\n",
    "print(\"kernel_receivals processing complete.\")\n",
    "df_kr_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6971fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing kernel_purchase_orders ---\n",
      "Removed non-positive quantity orders: Kept 33161 rows out of 33171.\n",
      "Removed columns: product_id, unit, unit_id, status_id, product_version.\n",
      "kernel_purchase_orders processing complete.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 33161 entries, 1 to 33170\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   purchase_order_id       33161 non-null  int64  \n",
      " 1   purchase_order_item_no  33161 non-null  int64  \n",
      " 2   quantity                33161 non-null  float64\n",
      " 3   delivery_date           33161 non-null  object \n",
      " 4   created_date_time       33161 non-null  object \n",
      " 5   modified_date_time      32669 non-null  object \n",
      " 6   status                  33161 non-null  object \n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Processing kernel_purchase_orders ---\")\n",
    "\n",
    "df_kpo_processed = df_kpo.copy()\n",
    "\n",
    "# Remove orders with quantity <= 0\n",
    "initial_rows = len(df_kpo_processed)\n",
    "df_kpo_processed = df_kpo_processed[df_kpo_processed[\"quantity\"] > 0]\n",
    "print(\n",
    "    f\"Removed non-positive quantity orders: Kept {len(df_kpo_processed)} rows out of {initial_rows}.\"\n",
    ")\n",
    "\n",
    "# Remove specified columns\n",
    "df_kpo_processed = df_kpo_processed.drop(\n",
    "    columns=[\"product_id\", \"unit\", \"unit_id\", \"status_id\", \"product_version\"]\n",
    ")\n",
    "print(\"Removed columns: product_id, unit, unit_id, status_id, product_version.\")\n",
    "\n",
    "print(\"kernel_purchase_orders processing complete.\")\n",
    "df_kpo_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd471c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing extended_materials ---\n",
      "Identified 29 rm_ids that are truly deleted.\n",
      "Removed columns: raw_material_alloy, product_id, stock_location.\n",
      "extended_materials processing complete.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 203 entries, 0 to 202\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   rm_id                     203 non-null    float64       \n",
      " 1   product_version           203 non-null    float64       \n",
      " 2   raw_material_format_type  203 non-null    float64       \n",
      " 3   is_deleted                203 non-null    bool          \n",
      " 4   deleted_date              0 non-null      datetime64[ns]\n",
      "dtypes: bool(1), datetime64[ns](1), float64(3)\n",
      "memory usage: 6.7 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Processing extended_materials ---\")\n",
    "\n",
    "df_em_processed = df_em.copy()\n",
    "\n",
    "# Filter for relevant rm_id's\n",
    "df_em_processed = df_em_processed[df_em_processed[\"rm_id\"].isin(relevant_rm_ids)]\n",
    "\n",
    "# --- Feature Engineering for DELETED stock locations ---\n",
    "\n",
    "# Identify rows where stock_location indicates deletion\n",
    "deleted_mask = df_em_processed[\"stock_location\"].str.startswith(\"DELETED\", na=False)\n",
    "deleted_entries = df_em_processed[deleted_mask]\n",
    "\n",
    "# Find rm_ids that ONLY have DELETED stock locations\n",
    "# First, get all rm_ids that have at least one DELETED entry\n",
    "rm_ids_with_deleted = deleted_entries[\"rm_id\"].unique()\n",
    "\n",
    "# Now, find which of those rm_ids ALSO have non-DELETED entries\n",
    "active_locations_for_same_rm_id = df_em_processed[\n",
    "    (df_em_processed[\"rm_id\"].isin(rm_ids_with_deleted)) & (~deleted_mask)\n",
    "]\n",
    "rm_ids_with_mixed_status = active_locations_for_same_rm_id[\"rm_id\"].unique()\n",
    "\n",
    "# The truly deleted rm_ids are those that are in the deleted list but NOT in the mixed list\n",
    "truly_deleted_rm_ids = set(rm_ids_with_deleted) - set(rm_ids_with_mixed_status)\n",
    "print(f\"Identified {len(truly_deleted_rm_ids)} rm_ids that are truly deleted.\")\n",
    "\n",
    "# Create the new boolean and date features\n",
    "df_em_processed[\"is_deleted\"] = df_em_processed[\"rm_id\"].isin(truly_deleted_rm_ids)\n",
    "\n",
    "# Extract the deleted date (this is a bit tricky, we'll use regex)\n",
    "# This extracts the YYYY-MM-DD date from strings like \"DELETED 2022-01-15 ...\"\n",
    "df_em_processed[\"deleted_date\"] = df_em_processed[\"stock_location\"].str.extract(\n",
    "    r\"(\\d{4}-\\d{2}-\\d{2})\"\n",
    ")\n",
    "df_em_processed[\"deleted_date\"] = pd.to_datetime(df_em_processed[\"deleted_date\"])\n",
    "\n",
    "# Only keep the deleted_date for truly deleted materials\n",
    "df_em_processed.loc[~df_em_processed[\"is_deleted\"], \"deleted_date\"] = pd.NaT\n",
    "\n",
    "\n",
    "# Remove specified columns\n",
    "df_em_processed = df_em_processed.drop(\n",
    "    columns=[\"raw_material_alloy\", \"product_id\", \"stock_location\"]\n",
    ")\n",
    "print(\"Removed columns: raw_material_alloy, product_id, stock_location.\")\n",
    "\n",
    "# We might have multiple rows per rm_id now, so let's keep the most relevant one.\n",
    "# We can group by rm_id and take the first entry, as the deleted flags are now consistent per rm_id.\n",
    "df_em_processed = df_em_processed.groupby(\"rm_id\").first().reset_index()\n",
    "\n",
    "\n",
    "print(\"extended_materials processing complete.\")\n",
    "df_em_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb7f8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing extended_transportation ---\n",
      "Removed columns: product_id, unit_status, wood, ironbands, plastic, water, ice, other, chips, packaging, cardboard.\n",
      "extended_transportation processing complete.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 122533 entries, 0 to 122589\n",
      "Data columns (total 12 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   rm_id                   122533 non-null  float64\n",
      " 1   purchase_order_id       122533 non-null  float64\n",
      " 2   purchase_order_item_no  122533 non-null  float64\n",
      " 3   receival_item_no        122533 non-null  int64  \n",
      " 4   batch_id                64716 non-null   float64\n",
      " 5   transporter_name        122533 non-null  object \n",
      " 6   vehicle_no              122533 non-null  object \n",
      " 7   vehicle_start_weight    122518 non-null  float64\n",
      " 8   vehicle_end_weight      122496 non-null  float64\n",
      " 9   gross_weight            122520 non-null  float64\n",
      " 10  tare_weight             108049 non-null  float64\n",
      " 11  net_weight              122520 non-null  float64\n",
      "dtypes: float64(9), int64(1), object(2)\n",
      "memory usage: 12.2+ MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Processing extended_transportation ---\")\n",
    "\n",
    "df_et_processed = df_et.copy()\n",
    "\n",
    "# Filter for relevant rm_id's\n",
    "df_et_processed = df_et_processed[df_et_processed[\"rm_id\"].isin(relevant_rm_ids)]\n",
    "\n",
    "# Remove specified columns\n",
    "columns_to_drop = [\n",
    "    \"product_id\",\n",
    "    \"unit_status\",\n",
    "    \"wood\",\n",
    "    \"ironbands\",\n",
    "    \"plastic\",\n",
    "    \"water\",\n",
    "    \"ice\",\n",
    "    \"other\",\n",
    "    \"chips\",\n",
    "    \"packaging\",\n",
    "    \"cardboard\",\n",
    "]\n",
    "df_et_processed = df_et_processed.drop(columns=columns_to_drop)\n",
    "print(f\"Removed columns: {', '.join(columns_to_drop)}.\")\n",
    "\n",
    "print(\"extended_transportation processing complete.\")\n",
    "df_et_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "191c824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Joining into master_df ---\n",
      "Joined receivals and purchase orders. Shape: (122533, 12)\n",
      "Joined materials data. Shape: (122533, 16)\n",
      "Joined transportation data. Shape: (2782749, 25)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2782749 entries, 0 to 2782748\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Dtype         \n",
      "---  ------                      -----         \n",
      " 0   rm_id                       float64       \n",
      " 1   purchase_order_id           float64       \n",
      " 2   purchase_order_item_no      float64       \n",
      " 3   receival_item_no            int64         \n",
      " 4   date_arrival                object        \n",
      " 5   net_weight                  float64       \n",
      " 6   supplier_id                 int64         \n",
      " 7   quantity                    float64       \n",
      " 8   delivery_date               object        \n",
      " 9   created_date_time           object        \n",
      " 10  modified_date_time          object        \n",
      " 11  status                      object        \n",
      " 12  product_version             float64       \n",
      " 13  raw_material_format_type    float64       \n",
      " 14  is_deleted                  bool          \n",
      " 15  deleted_date                datetime64[ns]\n",
      " 16  receival_item_no_transport  int64         \n",
      " 17  batch_id                    float64       \n",
      " 18  transporter_name            object        \n",
      " 19  vehicle_no                  object        \n",
      " 20  vehicle_start_weight        float64       \n",
      " 21  vehicle_end_weight          float64       \n",
      " 22  gross_weight                float64       \n",
      " 23  tare_weight                 float64       \n",
      " 24  net_weight_transport        float64       \n",
      "dtypes: bool(1), datetime64[ns](1), float64(13), int64(3), object(7)\n",
      "memory usage: 512.2+ MB\n",
      "Data saved successfully to /home/kfkh/Code/tdt4173-course-project/data/2_interim/master_data.csv ✅\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Joining into master_df ---\")\n",
    "\n",
    "# Use a composite key for joining purchase orders and transportation data\n",
    "# to handle cases where a single order has multiple items or receivals.\n",
    "key_cols = [\"purchase_order_id\", \"purchase_order_item_no\"]\n",
    "\n",
    "# Start with receivals as the base (the \"fact\" table)\n",
    "master_df = pd.merge(df_kr_processed, df_kpo_processed, on=key_cols, how=\"left\")\n",
    "print(f\"Joined receivals and purchase orders. Shape: {master_df.shape}\")\n",
    "\n",
    "# Merge materials data (on rm_id)\n",
    "master_df = pd.merge(master_df, df_em_processed, on=\"rm_id\", how=\"left\")\n",
    "print(f\"Joined materials data. Shape: {master_df.shape}\")\n",
    "\n",
    "# Merge transportation data\n",
    "# Note: Transportation data might have more keys, e.g., receival_item_no.\n",
    "# For simplicity, we'll join on the primary keys. You may need to refine this join\n",
    "# if you find it creates duplicates.\n",
    "transport_key_cols = key_cols + [\"rm_id\"]\n",
    "master_df = pd.merge(\n",
    "    master_df,\n",
    "    df_et_processed,\n",
    "    on=transport_key_cols,\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_transport\"),\n",
    ")\n",
    "print(f\"Joined transportation data. Shape: {master_df.shape}\")\n",
    "\n",
    "\n",
    "# --- Final Sanity Checks ---\n",
    "master_df.info()\n",
    "\n",
    "# --- Save the Processed Data ---\n",
    "# Saving as parquet is faster and preserves data types better than CSV\n",
    "# save_data(master_df, \"master_data.parquet\", folder=\"2_interim\")\n",
    "# Or use the provided CSV saver\n",
    "save_data(master_df, \"master_data.csv\", folder=\"2_interim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "082ee870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from /home/kfkh/Code/tdt4173-course-project/data/2_interim/master_data.csv\n",
      "--- Feature Presence & Fill Grade Across All DataFrames ---\n",
      "+----------------------------+---------------+\n",
      "|        Feature Name        |  master_data  |\n",
      "+============================+===============+\n",
      "|          batch_id          |   ✅ 31.5%    |\n",
      "+----------------------------+---------------+\n",
      "|     created_date_time      |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|        date_arrival        |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|        deleted_date        |    ✅ 0.0%    |\n",
      "+----------------------------+---------------+\n",
      "|       delivery_date        |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|        gross_weight        |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|         is_deleted         |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|     modified_date_time     |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|         net_weight         |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|    net_weight_transport    |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|      product_version       |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|     purchase_order_id      |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|   purchase_order_item_no   |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|          quantity          |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|  raw_material_format_type  |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|      receival_item_no      |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "| receival_item_no_transport |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|           rm_id            |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|           status           |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|        supplier_id         |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|        tare_weight         |   ✅ 90.5%    |\n",
      "+----------------------------+---------------+\n",
      "|      transporter_name      |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|     vehicle_end_weight     |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|         vehicle_no         |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "|    vehicle_start_weight    |   ✅ 100.0%   |\n",
      "+----------------------------+---------------+\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Order datasets for iteration\n",
    "df_m = load_data(\"master_data.csv\", \"2_interim\")\n",
    "\n",
    "datasets = {\n",
    "    \"master_data\": df_m,\n",
    "}\n",
    "generate_feature_presence_table(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4173-course-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
