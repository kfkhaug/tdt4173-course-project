{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b92373",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "This notebook is meant for all data cleaning and processing. Feel free to add or change sections.\n",
    "\n",
    "Some Todos:\n",
    "- `kernel_receivals`\n",
    "    - Remove all rows where `rm_id`'s don't exist in `prediction_mapping`.\n",
    "    - Remove all rows in `kernel_purchase_orders` with `quantity  <= 0`.\n",
    "    - Remove `product_id` and `receival_status`.\n",
    "    - Remove `batch_id`. It appears only from 2015 and onwards, and it has under 50% fill grade. This leads to poor data quality. In addition, it does not give predicitive information as all IDs are unique, and this can lead to overfitting in boosting-methods.\n",
    "- `kernel_purchase_orders`\n",
    "    - Remove `product_id`. There are multiple `rm_id`'s for each product_id, therefore it is useless here. We will know the rm_id after the dataset is joined with kernel_receivals based on purchase_order_id and/or purchase_order_item_no.\n",
    "    - Remove `unit` and `unit_id`. Almost all entries have KG and the ones that do not are simply NaN and can be assumed to be KG as well.\n",
    "    - Remove `status_id`. The status is duplicated in `status`.\n",
    "    - Remove `product_version`. It is just set to 1 for all orders up until 2016 --> Poor data quality.\n",
    "- `extended_materials`\n",
    "    - Remove all rows where `rm_id`'s don't exist in `prediction_mapping`.\n",
    "    - Remove `raw_material_alloy`, `product_id`.\n",
    "    - There are a good deal of entries where `stock_location` is prefixed with DELETED followed by a date. It would be useful to check if there are differences in deliveries before and after that date. My hypothesis is that after it is deleted, there are no more deliveries for that rm_id. If this is a case, a boolean feature `is_deleted` would be powerful. Before inserting a DELETED=TRUE tag for an rm_id, check that there are no other listings of the same rm_id's that do not have DELETED in the stock_location variable. If there are other non-DELETED locations, then it may be that the stock location has simply moved but is still active. Be sure to also make a `deleted_date` feature as well. Then it is possible to create a `days_since_deleted = (forecast_date - deleted_date)` feature.\n",
    "    - Remove `stock_location`, but only after making the `is_deleted` boolean and`date_deleted`.\n",
    "- `extended_transportation`\n",
    "    - Remove all rows where `rm_id`'s don't exist in `prediction_mapping`.\n",
    "    - Remove `product_id`.\n",
    "    - Remove `unit_status`. Almost all entries are 'Tranferred', a some are 'Accepted' and a couple are 'Pending'.\n",
    "    - Remove `wood`, `ironbands`, `plastic`, `water`, `ice`, `other`, `chips`, `packaging`, `cardboard`. These have very few entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d5deb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90550889",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da89dc5e",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e51765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, folder=\"1_raw\"):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file in a subfolder of the project's 'data' directory.\n",
    "    This version is adjusted to work even if the notebook is run from a subfolder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The name of the file to load, including the extension (e.g., \"data.csv\").\n",
    "    folder : str, optional\n",
    "        The subfolder within 'data' to load from. Defaults to \"1_raw\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Go up one level from the current working directory to find the project root\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "        file_path = PROJECT_ROOT / \"data\" / folder / filename\n",
    "\n",
    "        df = pd.read_csv(file_path, sep=\",\")\n",
    "\n",
    "        print(f\"Data loaded successfully from {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_data(df, filename, folder=\"2_interim\"):\n",
    "    \"\"\"\n",
    "    Save a dataframe to a CSV file in a subfolder of the project's 'data' directory.\n",
    "\n",
    "    This function automatically creates the destination folder if it doesn't exist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to save.\n",
    "    filename : str\n",
    "        The name for the output file, including the extension (e.g., \"processed_orders.csv\").\n",
    "    folder : str, optional\n",
    "        The subfolder within 'data' to save to. Defaults to \"2_interim\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "        save_dir = PROJECT_ROOT / \"data\" / folder\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # The full filename, including extension, is now expected\n",
    "        file_path = save_dir / filename\n",
    "\n",
    "        df.to_csv(file_path, sep=\",\", index=False)\n",
    "\n",
    "        print(f\"Data saved successfully to {file_path} ✅\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the file: {e}\")\n",
    "\n",
    "\n",
    "def generate_feature_presence_table(datasets: Dict[str, pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    Analyzes a dictionary of pandas DataFrames to determine the presence and\n",
    "    fill grade of features (columns) across all of them. It then prints a\n",
    "    formatted table summarizing this information.\n",
    "\n",
    "    Args:\n",
    "        datasets (Dict[str, pd.DataFrame]): A dictionary where keys are string\n",
    "            names for the DataFrames and values are the pandas DataFrames\n",
    "            themselves.\n",
    "    \"\"\"\n",
    "    if not isinstance(datasets, dict) or not all(\n",
    "        isinstance(df, pd.DataFrame) for df in datasets.values()\n",
    "    ):\n",
    "        print(\"Error: Input must be a dictionary of pandas DataFrames.\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Identify all unique features across all DataFrames ---\n",
    "    all_features = sorted(\n",
    "        list(set(feature for df in datasets.values() for feature in df.columns))\n",
    "    )\n",
    "\n",
    "    # --- 2. Build the presence and fill grade data for each DataFrame ---\n",
    "    presence_data = []\n",
    "    for name, df in datasets.items():\n",
    "        row = {\"DataFrame Name\": name}\n",
    "        for feature in all_features:\n",
    "            if feature in df.columns:\n",
    "                series = df[feature]\n",
    "                total_count = len(series)\n",
    "\n",
    "                if total_count > 0:\n",
    "                    non_nan_count = series.count()  # .count() excludes NaNs\n",
    "                    fill_grade = (non_nan_count / total_count) * 100\n",
    "                    row[feature] = f\"✅ {fill_grade:.1f}%\"\n",
    "                else:\n",
    "                    # Handle empty DataFrames/Series\n",
    "                    row[feature] = \"✅ 100.0%\"\n",
    "            else:\n",
    "                # Feature is not present in this DataFrame\n",
    "                row[feature] = \"\"\n",
    "        presence_data.append(row)\n",
    "\n",
    "    # --- 3. Create, transpose, and format the DataFrame for display ---\n",
    "    if not presence_data:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "\n",
    "    presence_df = pd.DataFrame(presence_data)\n",
    "    presence_df = presence_df.set_index(\"DataFrame Name\")\n",
    "\n",
    "    # Transpose the DataFrame so features are rows and datasets are columns\n",
    "    presence_df_transposed = presence_df.T\n",
    "    presence_df_transposed.index.name = \"Feature Name\"\n",
    "\n",
    "    # --- 4. Print the final table using tabulate ---\n",
    "    print(\"--- Feature Presence & Fill Grade Across All DataFrames ---\")\n",
    "    print(\n",
    "        tabulate(\n",
    "            presence_df_transposed, headers=\"keys\", tablefmt=\"grid\", stralign=\"center\"\n",
    "        )\n",
    "    )\n",
    "    print(\"\\n\" * 2)  # Add some space after the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febc650",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6963740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel files\n",
    "df_kpo = load_data(\"kernel_purchase_orders.csv\")\n",
    "df_kr = load_data(\"kernel_receivals.csv\")\n",
    "\n",
    "# Extended files\n",
    "df_em = load_data(\"extended_materials.csv\")\n",
    "df_et = load_data(\"extended_transportation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6300ad",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd785a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order datasets for iteration\n",
    "datasets = {\n",
    "    \"kernel_receivals\": df_kr,\n",
    "    \"kernel_purchase_orders\": df_kpo,\n",
    "    \"extended_materials\": df_em,\n",
    "    \"extended_transportation\": df_et,\n",
    "}\n",
    "generate_feature_presence_table(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mapping file to get the relevant rm_id's\n",
    "df_map = load_data(\"prediction_mapping.csv\")\n",
    "relevant_rm_ids = df_map[\"rm_id\"].unique()\n",
    "print(f\"Found {len(relevant_rm_ids)} unique rm_id's to focus on.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Processing kernel_receivals ---\")\n",
    "\n",
    "# Start with a copy to avoid modifying the original dataframe\n",
    "df_kr_processed = df_kr.copy()\n",
    "\n",
    "# Filter for relevant rm_id's\n",
    "initial_rows = len(df_kr_processed)\n",
    "df_kr_processed = df_kr_processed[df_kr_processed[\"rm_id\"].isin(relevant_rm_ids)]\n",
    "print(f\"Filtered rm_ids: Kept {len(df_kr_processed)} rows out of {initial_rows}.\")\n",
    "\n",
    "# Remove specified columns\n",
    "df_kr_processed = df_kr_processed.drop(\n",
    "    columns=[\"product_id\", \"receival_status\", \"batch_id\"]\n",
    ")\n",
    "print(\"Removed columns: product_id, receival_status, batch_id.\")\n",
    "\n",
    "# Ensure date_arrival is a datetime object for future joins and feature engineering\n",
    "df_kr_processed[\"date_arrival\"] = pd.to_datetime(df_kr_processed[\"date_arrival\"])\n",
    "\n",
    "print(\"kernel_receivals processing complete.\")\n",
    "df_kr_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6971fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Processing kernel_purchase_orders ---\")\n",
    "\n",
    "df_kpo_processed = df_kpo.copy()\n",
    "\n",
    "# Remove orders with quantity <= 0\n",
    "initial_rows = len(df_kpo_processed)\n",
    "df_kpo_processed = df_kpo_processed[df_kpo_processed[\"quantity\"] > 0]\n",
    "print(\n",
    "    f\"Removed non-positive quantity orders: Kept {len(df_kpo_processed)} rows out of {initial_rows}.\"\n",
    ")\n",
    "\n",
    "# Remove specified columns\n",
    "df_kpo_processed = df_kpo_processed.drop(\n",
    "    columns=[\"product_id\", \"unit\", \"unit_id\", \"status_id\", \"product_version\"]\n",
    ")\n",
    "print(\"Removed columns: product_id, unit, unit_id, status_id, product_version.\")\n",
    "\n",
    "print(\"kernel_purchase_orders processing complete.\")\n",
    "df_kpo_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd471c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Processing extended_materials ---\")\n",
    "\n",
    "df_em_processed = df_em.copy()\n",
    "\n",
    "# Filter for relevant rm_id's\n",
    "df_em_processed = df_em_processed[df_em_processed[\"rm_id\"].isin(relevant_rm_ids)]\n",
    "\n",
    "# --- Feature Engineering for DELETED stock locations ---\n",
    "\n",
    "# Identify rows where stock_location indicates deletion\n",
    "deleted_mask = df_em_processed[\"stock_location\"].str.startswith(\"DELETED\", na=False)\n",
    "deleted_entries = df_em_processed[deleted_mask]\n",
    "\n",
    "# Find rm_ids that ONLY have DELETED stock locations\n",
    "# First, get all rm_ids that have at least one DELETED entry\n",
    "rm_ids_with_deleted = deleted_entries[\"rm_id\"].unique()\n",
    "\n",
    "# Now, find which of those rm_ids ALSO have non-DELETED entries\n",
    "active_locations_for_same_rm_id = df_em_processed[\n",
    "    (df_em_processed[\"rm_id\"].isin(rm_ids_with_deleted)) & (~deleted_mask)\n",
    "]\n",
    "rm_ids_with_mixed_status = active_locations_for_same_rm_id[\"rm_id\"].unique()\n",
    "\n",
    "# The truly deleted rm_ids are those that are in the deleted list but NOT in the mixed list\n",
    "truly_deleted_rm_ids = set(rm_ids_with_deleted) - set(rm_ids_with_mixed_status)\n",
    "print(f\"Identified {len(truly_deleted_rm_ids)} rm_ids that are truly deleted.\")\n",
    "\n",
    "# Create the new boolean and date features\n",
    "df_em_processed[\"is_deleted\"] = df_em_processed[\"rm_id\"].isin(truly_deleted_rm_ids)\n",
    "\n",
    "# Extract the deleted date (this is a bit tricky, we'll use regex)\n",
    "# This extracts the YYYY-MM-DD date from strings like \"DELETED 2022-01-15 ...\"\n",
    "df_em_processed[\"deleted_date\"] = df_em_processed[\"stock_location\"].str.extract(\n",
    "    r\"(\\d{4}-\\d{2}-\\d{2})\"\n",
    ")\n",
    "df_em_processed[\"deleted_date\"] = pd.to_datetime(df_em_processed[\"deleted_date\"])\n",
    "\n",
    "# Only keep the deleted_date for truly deleted materials\n",
    "df_em_processed.loc[~df_em_processed[\"is_deleted\"], \"deleted_date\"] = pd.NaT\n",
    "\n",
    "\n",
    "# Remove specified columns\n",
    "df_em_processed = df_em_processed.drop(\n",
    "    columns=[\"raw_material_alloy\", \"product_id\", \"stock_location\"]\n",
    ")\n",
    "print(\"Removed columns: raw_material_alloy, product_id, stock_location.\")\n",
    "\n",
    "# We might have multiple rows per rm_id now, so let's keep the most relevant one.\n",
    "# We can group by rm_id and take the first entry, as the deleted flags are now consistent per rm_id.\n",
    "df_em_processed = df_em_processed.groupby(\"rm_id\").first().reset_index()\n",
    "\n",
    "\n",
    "print(\"extended_materials processing complete.\")\n",
    "df_em_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Processing extended_transportation ---\")\n",
    "\n",
    "df_et_processed = df_et.copy()\n",
    "\n",
    "# Filter for relevant rm_id's\n",
    "df_et_processed = df_et_processed[df_et_processed[\"rm_id\"].isin(relevant_rm_ids)]\n",
    "\n",
    "# Remove specified columns\n",
    "columns_to_drop = [\n",
    "    \"product_id\",\n",
    "    \"unit_status\",\n",
    "    \"wood\",\n",
    "    \"ironbands\",\n",
    "    \"plastic\",\n",
    "    \"water\",\n",
    "    \"ice\",\n",
    "    \"other\",\n",
    "    \"chips\",\n",
    "    \"packaging\",\n",
    "    \"cardboard\",\n",
    "]\n",
    "df_et_processed = df_et_processed.drop(columns=columns_to_drop)\n",
    "print(f\"Removed columns: {', '.join(columns_to_drop)}.\")\n",
    "\n",
    "print(\"extended_transportation processing complete.\")\n",
    "df_et_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Joining into master_df ---\")\n",
    "\n",
    "# Use a composite key for joining purchase orders and transportation data\n",
    "# to handle cases where a single order has multiple items or receivals.\n",
    "key_cols = [\"purchase_order_id\", \"purchase_order_item_no\"]\n",
    "\n",
    "# Start with receivals as the base (the \"fact\" table)\n",
    "master_df = pd.merge(df_kr_processed, df_kpo_processed, on=key_cols, how=\"left\")\n",
    "print(f\"Joined receivals and purchase orders. Shape: {master_df.shape}\")\n",
    "\n",
    "# Merge materials data (on rm_id)\n",
    "master_df = pd.merge(master_df, df_em_processed, on=\"rm_id\", how=\"left\")\n",
    "print(f\"Joined materials data. Shape: {master_df.shape}\")\n",
    "\n",
    "# Merge transportation data\n",
    "# Note: Transportation data might have more keys, e.g., receival_item_no.\n",
    "# For simplicity, we'll join on the primary keys. You may need to refine this join\n",
    "# if you find it creates duplicates.\n",
    "transport_key_cols = key_cols + [\"rm_id\"]\n",
    "master_df = pd.merge(\n",
    "    master_df,\n",
    "    df_et_processed,\n",
    "    on=transport_key_cols,\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_transport\"),\n",
    ")\n",
    "print(f\"Joined transportation data. Shape: {master_df.shape}\")\n",
    "\n",
    "# --- Final Sanity Checks ---\n",
    "master_df.info()\n",
    "\n",
    "# --- FORMAT `master_df` FOR LIGHTGBM ---\n",
    "\n",
    "def _parse_datetime_utc_series(s: pd.Series) -> pd.Series:\n",
    "    # Normalize any \" +00:00\" artifacts to UTC datetimes\n",
    "    s = s.astype(\"string\").str.replace(r\"\\s\\+\", \"+\", regex=True)\n",
    "    return pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
    "\n",
    "def _to_int_if_whole_or_float_series(s: pd.Series) -> pd.Series:\n",
    "    # Cast to Int32 if all non-null values are whole numbers, otherwise float32\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if (x.dropna() % 1 == 0).all():\n",
    "        return x.astype(\"Int32\")\n",
    "    else:\n",
    "        return x.astype(\"float32\")\n",
    "\n",
    "# Column groups\n",
    "date_cols = [\"date_arrival\", \"delivery_date\", \"created_date_time\", \"modified_date_time\", \"deleted_date\"]\n",
    "id_cols   = [\"rm_id\", \"purchase_order_id\", \"purchase_order_item_no\", \"receival_item_no\", \"receival_item_no_transport\"]  # batch_id removed\n",
    "bool_cols = [\"is_deleted\"]\n",
    "maybe_int_or_float = [\n",
    "    \"net_weight\", \"quantity\", \"product_version\", \"vehicle_start_weight\",\n",
    "    \"vehicle_end_weight\", \"gross_weight\", \"tare_weight\", \"net_weight_transport\"\n",
    "]\n",
    "# Treat these as categorical features (even if they look numeric)\n",
    "cat_cols  = [\"supplier_id\", \"status\", \"raw_material_format_type\", \"transporter_name\", \"vehicle_no\"]\n",
    "\n",
    "_df = master_df.copy()\n",
    "\n",
    "# Drop batch_id entirely if present\n",
    "if \"batch_id\" in _df.columns:\n",
    "    _df = _df.drop(columns=[\"batch_id\"])\n",
    "\n",
    "# Keep only columns that exist\n",
    "date_cols = [c for c in date_cols if c in _df.columns]\n",
    "id_cols   = [c for c in id_cols   if c in _df.columns]\n",
    "bool_cols = [c for c in bool_cols if c in _df.columns]\n",
    "maybe_int_or_float = [c for c in maybe_int_or_float if c in _df.columns]\n",
    "cat_cols  = [c for c in cat_cols  if c in _df.columns]\n",
    "\n",
    "# 1) Datetimes -> UTC\n",
    "for c in date_cols:\n",
    "    _df[c] = _parse_datetime_utc_series(_df[c])\n",
    "\n",
    "# 2) Booleans\n",
    "for c in bool_cols:\n",
    "    _df[c] = (\n",
    "        _df[c].astype(\"string\").str.strip().str.lower()\n",
    "        .map({\"true\": True, \"false\": False, \"1\": True, \"0\": False})\n",
    "        .astype(\"boolean\")\n",
    "    )\n",
    "\n",
    "# 3) Numeric smart cast\n",
    "for c in maybe_int_or_float:\n",
    "    _df[c] = _to_int_if_whole_or_float_series(_df[c])\n",
    "\n",
    "# 3b) Force 'quantity' to nullable Int64\n",
    "if \"quantity\" in _df.columns:\n",
    "    q = pd.to_numeric(_df[\"quantity\"], errors=\"coerce\")\n",
    "    _df[\"quantity\"] = q.round().astype(\"Int64\")\n",
    "\n",
    "# 4) IDs -> nullable Int64\n",
    "for c in id_cols:\n",
    "    _df[c] = pd.to_numeric(_df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# 5) Categoricals -> int codes. store mapping (unknown -> -1)\n",
    "cat_maps = {}\n",
    "for c in cat_cols:\n",
    "    _df[c] = _df[c].astype(\"string\").str.strip().replace({\"\": pd.NA}).astype(\"category\")\n",
    "    categories = list(_df[c].cat.categories.astype(\"string\"))\n",
    "    mapping = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    codes = _df[c].cat.codes.astype(\"Int32\")  # pandas uses -1 for NaN\n",
    "    _df[c] = codes\n",
    "    cat_maps[c] = {\"mapping\": mapping, \"unknown_value\": -1}\n",
    "\n",
    "# 6) Report constant columns (excluding IDs)\n",
    "constant_cols = [c for c in _df.columns if _df[c].nunique(dropna=True) <= 1 and c not in id_cols]\n",
    "if len(constant_cols):\n",
    "    print(\"Constant columns:\", constant_cols)\n",
    "\n",
    "# 7) Persist artifacts (dtype report + category maps)\n",
    "art_dir = Path(\"./artifacts\")\n",
    "art_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(art_dir / \"master_data_dtype_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({c: str(_df[c].dtype) for c in _df.columns}, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(art_dir / \"category_maps.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cat_maps, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# --- Row ordering ---\n",
    "\n",
    "def _coerce_rm_id_for_sort(s):\n",
    "    # Force numeric\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _coerce_dt_for_sort(s):\n",
    "    # Parse with UTC\n",
    "    return pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
    "\n",
    "# Choose delivery date column\n",
    "_dt_col = None\n",
    "for candidate in [\"delivery_date\", \"date_delivery\", \"agreed_delivery_date\"]:\n",
    "    if candidate in _df.columns:\n",
    "        _dt_col = candidate\n",
    "        break\n",
    "if _dt_col is None:\n",
    "    raise KeyError(\"Expected a delivery date column was not found.\")\n",
    "\n",
    "# Build temporary sort keys to avoid dtype/categorical issues\n",
    "_rm_key = _coerce_rm_id_for_sort(_df[\"rm_id\"])\n",
    "_dt_key = _coerce_dt_for_sort(_df[_dt_col])\n",
    "\n",
    "# Stable sort: rm_id ascending, delivery_date descending\n",
    "_df_sorted = (\n",
    "    _df.assign(_k_rm=_rm_key, _k_dt=_dt_key)\n",
    "       .sort_values([\"_k_rm\", \"_k_dt\"], ascending=[True, False],\n",
    "                    kind=\"mergesort\", na_position=\"last\")\n",
    "       .drop(columns=[\"_k_rm\", \"_k_dt\"])\n",
    "       .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Finalize for downstream code\n",
    "master_df = _df_sorted\n",
    "try:\n",
    "    master_data = master_df  # if other parts expect this name\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"master_df formatted. Dtypes:\\n\", master_df.dtypes)\n",
    "\n",
    "# --- Save the Processed Data ---\n",
    "# Parquet preserves dtypes better; CSV is fine if required by your pipeline\n",
    "# save_data(master_df, \"master_data.parquet\", folder=\"2_interim\")\n",
    "save_data(master_df, \"master_data.csv\", folder=\"2_interim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ee870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order datasets for iteration\n",
    "df_m = load_data(\"master_data.csv\", \"2_interim\")\n",
    "\n",
    "datasets = {\n",
    "    \"master_data\": df_m,\n",
    "}\n",
    "generate_feature_presence_table(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4173-course-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
