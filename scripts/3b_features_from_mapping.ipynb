{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d5d2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE ENGINEERING PIPELINE\n",
      "TDT4173 Course Project - Hydro ASA x Append Consulting\n",
      "======================================================================\n",
      "\n",
      "[1/5] Reading input data...\n",
      "  Prediction mapping: 30450 rows\n",
      "  Master receival data: 40943 rows\n",
      "  Unique raw materials: 203\n",
      "\n",
      "[2/5] Building daily aggregated features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timjo\\AppData\\Local\\Temp\\ipykernel_25780\\3477421370.py:138: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(complete_calendar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Daily features: 223606 rows\n",
      "  Date range: 2004-06-15 00:00:00 to 2024-12-19 00:00:00\n",
      "\n",
      "[3/5] Reading purchase orders...\n",
      " Purchase orders: 27425 rows\n",
      "\n",
      "[4/5] Generating submission features...\n",
      "  ✓ Wrote 30450 rows -> c:\\Users\\timjo\\ML\\tdt4173-course-project\\data\\3_processed\\features_for_submission.csv\n",
      "\n",
      "[5/5] Generating training features and labels...\n",
      " Training windows: 100000 rows\n",
      " Wrote 100000 rows -> c:\\Users\\timjo\\ML\\tdt4173-course-project\\data\\3_processed\\training_features_and_labels.csv\n",
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature Engineering Pipeline for Raw Material Delivery Prediction\n",
    "TDT4173 Course Project - Hydro ASA x Append Consulting\n",
    "\n",
    "This script builds features for predicting cumulative raw material deliveries.\n",
    "It generates both submission features and training data with proper time-aware splits.\n",
    "\n",
    "Key Features:\n",
    "- Rolling window statistics (7, 14, 28, 56, 365 days)\n",
    "- Same period last year comparisons\n",
    "- Purchase order features (respecting temporal data leakage)\n",
    "- Calendar features (month, day of week)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# PATH CONFIGURATION\n",
    "# ============================================================================\n",
    "DATA = Path.cwd() / \"data\"\n",
    "if not DATA.exists():\n",
    "    DATA = Path.cwd().parent / \"data\"\n",
    "\n",
    "RAW = DATA / \"1_raw\"\n",
    "INTERIM = DATA / \"2_interim\"\n",
    "PROCESSED = DATA / \"3_processed\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "INTERIM.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input files\n",
    "PREDICTION_MAPPING_CSV = RAW / \"prediction_mapping.csv\"\n",
    "MASTER_DATA_CSV = INTERIM / \"master_data.csv\"\n",
    "PURCHASE_ORDERS_CSV = RAW / \"kernel_purchase_orders.csv\"\n",
    "\n",
    "# Output files\n",
    "FEATURES_SUBMISSION = PROCESSED / \"features_for_submission.csv\"\n",
    "FEATURES_TRAINING = PROCESSED / \"training_features_and_labels.csv\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def read_mapping(mapping_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read and normalize the prediction mapping file.\n",
    "    \n",
    "    Returns DataFrame with columns: ID, rm_id, forecast_start_date, forecast_end_date\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        mapping_path,\n",
    "        parse_dates=[\"forecast_start_date\", \"forecast_end_date\"]\n",
    "    )\n",
    "    df = df[[\"ID\", \"rm_id\", \"forecast_start_date\", \"forecast_end_date\"]].copy()\n",
    "    df[\"rm_id\"] = pd.to_numeric(df[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_master(master_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read and preprocess master receival data.\n",
    "    \n",
    "    Handles duplicates and normalizes date/weight columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(master_path, low_memory=False)\n",
    "    \n",
    "    # Normalize dates and weights\n",
    "    df[\"date_arrival\"] = pd.to_datetime(\n",
    "        df[\"date_arrival\"], utc=True, errors=\"coerce\"\n",
    "    ).dt.tz_convert(None)\n",
    "    df[\"net_weight\"] = pd.to_numeric(\n",
    "        df.get(\"net_weight\"), errors=\"coerce\"\n",
    "    ).fillna(0.0)\n",
    "    \n",
    "    # Normalize ID columns\n",
    "    for col in [\"rm_id\", \"purchase_order_id\", \"purchase_order_item_no\", \"receival_item_no\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "    \n",
    "    # Remove duplicates based on available keys\n",
    "    recv_key = [\"rm_id\", \"purchase_order_id\", \"purchase_order_item_no\", \"receival_item_no\"]\n",
    "    if not set(recv_key).issubset(df.columns):\n",
    "        recv_key = [\"rm_id\", \"date_arrival\", \"net_weight\"]\n",
    "    df = df.drop_duplicates(subset=recv_key, keep=\"first\").copy()\n",
    "    \n",
    "    # Create normalized date column (day-level)\n",
    "    df[\"rm_id\"] = pd.to_numeric(df[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date_arrival\"].dt.date)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def complete_calendar(daily_rm: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fill missing dates with zero weight for a single rm_id group.\n",
    "    \n",
    "    This ensures we have data for every day in the range, enabling exact date joins.\n",
    "    \"\"\"\n",
    "    full = pd.DataFrame({\n",
    "        \"date\": pd.date_range(\n",
    "            daily_rm[\"date\"].min(),\n",
    "            daily_rm[\"date\"].max(),\n",
    "            freq=\"D\"\n",
    "        )\n",
    "    })\n",
    "    full[\"rm_id\"] = daily_rm.name\n",
    "    out = full.merge(daily_rm, on=[\"rm_id\", \"date\"], how=\"left\")\n",
    "    out[\"net_weight\"] = out[\"net_weight\"].fillna(0.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_daily(master_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build daily aggregated features for each raw material.\n",
    "    \n",
    "    Features include:\n",
    "    - Cumulative weight\n",
    "    - Rolling window statistics (7, 14, 28, 56, 365 days)\n",
    "    - Days since last delivery\n",
    "    - Mean kg per delivery day\n",
    "    \"\"\"\n",
    "    master_df = master_df.copy()\n",
    "    \n",
    "    # Aggregate to daily level\n",
    "    daily = (\n",
    "        master_df.groupby([\"rm_id\", \"date\"], as_index=False)[\"net_weight\"]\n",
    "        .sum()\n",
    "        .sort_values([\"rm_id\", \"date\"])\n",
    "    )\n",
    "    \n",
    "    # Fill complete calendar for each rm_id\n",
    "    daily = (\n",
    "        daily.groupby(\"rm_id\", group_keys=False)\n",
    "        .apply(complete_calendar)\n",
    "        .sort_values([\"rm_id\", \"date\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Cumulative weight\n",
    "    daily[\"cum_kg\"] = daily.groupby(\"rm_id\")[\"net_weight\"].cumsum()\n",
    "    \n",
    "    # Delivery flag (1 if delivery, 0 otherwise)\n",
    "    daily[\"deliveries_flag\"] = (daily[\"net_weight\"] > 0).astype(int)\n",
    "    \n",
    "    # Rolling window features\n",
    "    for window in [7, 14, 28, 56, 365]:\n",
    "        daily[f\"r{window}_kg\"] = (\n",
    "            daily.groupby(\"rm_id\")[\"net_weight\"]\n",
    "            .rolling(window, min_periods=1)\n",
    "            .sum()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        daily[f\"r{window}_days\"] = (\n",
    "            daily.groupby(\"rm_id\")[\"deliveries_flag\"]\n",
    "            .rolling(window, min_periods=1)\n",
    "            .sum()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Mean kg per delivery day (avoid division by zero)\n",
    "    def safe_div(a, b):\n",
    "        b_safe = b.replace(0, np.nan)\n",
    "        return (a / b_safe).fillna(0.0)\n",
    "    \n",
    "    daily[\"r28_mean_kg_per_day\"] = safe_div(daily[\"r28_kg\"], daily[\"r28_days\"])\n",
    "    daily[\"r56_mean_kg_per_day\"] = safe_div(daily[\"r56_kg\"], daily[\"r56_days\"])\n",
    "    \n",
    "    # Days since last delivery\n",
    "    tmp = daily[[\"rm_id\", \"date\", \"net_weight\"]].copy()\n",
    "    tmp[\"last_deliv_date\"] = tmp[\"date\"].where(tmp[\"net_weight\"] > 0)\n",
    "    tmp[\"last_deliv_date\"] = tmp.groupby(\"rm_id\")[\"last_deliv_date\"].ffill()\n",
    "    daily[\"days_since_last\"] = (\n",
    "        (daily[\"date\"] - tmp[\"last_deliv_date\"])\n",
    "        .dt.days.fillna(10_000)\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    return daily\n",
    "\n",
    "\n",
    "def read_orders_with_rm(\n",
    "    orders_path: Path | None,\n",
    "    master_df: pd.DataFrame\n",
    ") -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Read purchase orders and join with rm_id from master data.\n",
    "    \n",
    "    Returns None if file doesn't exist.\n",
    "    \"\"\"\n",
    "    if orders_path is None or not orders_path.exists():\n",
    "        return None\n",
    "    \n",
    "    po = pd.read_csv(\n",
    "        orders_path,\n",
    "        low_memory=False,\n",
    "        parse_dates=[\"delivery_date\", \"created_date_time\", \"modified_date_time\"]\n",
    "    )\n",
    "    \n",
    "    # Normalize ID columns\n",
    "    for col in [\"purchase_order_id\", \"purchase_order_item_no\"]:\n",
    "        if col in po.columns:\n",
    "            po[col] = pd.to_numeric(po[col], errors=\"coerce\").astype(\"Int64\")\n",
    "    \n",
    "    po[\"quantity\"] = pd.to_numeric(po.get(\"quantity\"), errors=\"coerce\").fillna(0.0)\n",
    "    \n",
    "    # Join with master to get rm_id\n",
    "    rm_lookup = (\n",
    "        master_df[[\"rm_id\", \"purchase_order_id\", \"purchase_order_item_no\"]]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    po = po.merge(rm_lookup, on=[\"purchase_order_id\", \"purchase_order_item_no\"], how=\"left\")\n",
    "    \n",
    "    # Normalize dates\n",
    "    if \"delivery_date\" in po.columns:\n",
    "        po[\"delivery_date\"] = pd.to_datetime(\n",
    "            po[\"delivery_date\"], utc=True, errors=\"coerce\"\n",
    "        ).dt.tz_convert(None)\n",
    "    if \"created_date_time\" in po.columns:\n",
    "        po[\"created_date_time\"] = pd.to_datetime(\n",
    "            po[\"created_date_time\"], utc=True, errors=\"coerce\"\n",
    "        ).dt.tz_convert(None)\n",
    "    if \"modified_date_time\" in po.columns:\n",
    "        po[\"modified_date_time\"] = pd.to_datetime(\n",
    "            po[\"modified_date_time\"], utc=True, errors=\"coerce\"\n",
    "        ).dt.tz_convert(None)\n",
    "    \n",
    "    # Keep relevant columns\n",
    "    keep_cols = [\n",
    "        c for c in [\n",
    "            \"rm_id\", \"quantity\", \"delivery_date\",\n",
    "            \"created_date_time\", \"modified_date_time\",\n",
    "            \"purchase_order_id\", \"purchase_order_item_no\"\n",
    "        ]\n",
    "        if c in po.columns\n",
    "    ]\n",
    "    po = po[keep_cols].dropna(subset=[\"rm_id\", \"delivery_date\"]).copy()\n",
    "    po[\"rm_id\"] = pd.to_numeric(po[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    \n",
    "    return po\n",
    "\n",
    "\n",
    "def same_period_last_year(\n",
    "    df_map: pd.DataFrame,\n",
    "    csum: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate total weight delivered in the same period last year.\n",
    "    \n",
    "    Uses groupby + manual filtering approach to avoid merge_asof sorting issues.\n",
    "    For each forecast window, looks back 365-366 days and calculates the difference\n",
    "    in cumulative weight.\n",
    "    \"\"\"\n",
    "    df_map = df_map.copy()\n",
    "    df_map[\"rm_id\"] = pd.to_numeric(df_map[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    \n",
    "    # Prepare cumulative data\n",
    "    csum_clean = csum[[\"rm_id\", \"date\", \"cum_kg\"]].copy()\n",
    "    csum_clean[\"rm_id\"] = pd.to_numeric(\n",
    "        csum_clean[\"rm_id\"], errors=\"coerce\"\n",
    "    ).astype(\"int64\")\n",
    "    csum_clean[\"date\"] = pd.to_datetime(csum_clean[\"date\"], errors=\"coerce\")\n",
    "    csum_clean = csum_clean.dropna(subset=[\"rm_id\", \"date\"])\n",
    "    csum_clean = csum_clean.sort_values([\"rm_id\", \"date\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate dates from last year\n",
    "    df_map[\"prev_start\"] = df_map[\"forecast_start_date\"] - pd.Timedelta(days=366)\n",
    "    df_map[\"prev_end\"] = df_map[\"forecast_end_date\"] - pd.Timedelta(days=365)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each rm_id separately\n",
    "    for rm_id, group in df_map.groupby(\"rm_id\"):\n",
    "        rm_csum = csum_clean[csum_clean[\"rm_id\"] == rm_id].copy()\n",
    "        \n",
    "        if rm_csum.empty:\n",
    "            # No historical data for this rm_id\n",
    "            for _, row in group.iterrows():\n",
    "                results.append({\n",
    "                    \"ID\": row[\"ID\"],\n",
    "                    \"same_period_last_year_kg\": 0.0\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        # For each forecast window\n",
    "        for _, row in group.iterrows():\n",
    "            prev_start = row[\"prev_start\"]\n",
    "            prev_end = row[\"prev_end\"]\n",
    "            \n",
    "            # Find nearest date <= prev_start\n",
    "            before_start = rm_csum[rm_csum[\"date\"] <= prev_start]\n",
    "            cum_a = before_start[\"cum_kg\"].iloc[-1] if not before_start.empty else 0.0\n",
    "            \n",
    "            # Find nearest date <= prev_end\n",
    "            before_end = rm_csum[rm_csum[\"date\"] <= prev_end]\n",
    "            cum_b = before_end[\"cum_kg\"].iloc[-1] if not before_end.empty else 0.0\n",
    "            \n",
    "            # Calculate difference\n",
    "            diff = max(0.0, cum_b - cum_a)\n",
    "            results.append({\n",
    "                \"ID\": row[\"ID\"],\n",
    "                \"same_period_last_year_kg\": diff\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def add_orders_features(\n",
    "    df_map: pd.DataFrame,\n",
    "    orders: pd.DataFrame | None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add purchase order features while respecting temporal data leakage.\n",
    "    \n",
    "    Only includes orders that were:\n",
    "    1. Created before the cutoff date\n",
    "    2. Not modified after the cutoff date (or never modified)\n",
    "    3. Expected to deliver within the forecast window\n",
    "    \"\"\"\n",
    "    if orders is None or orders.empty:\n",
    "        df_map[\"orders_qty_in_window\"] = 0.0\n",
    "        df_map[\"orders_lines_in_window\"] = 0\n",
    "        return df_map\n",
    "    \n",
    "    # Join orders with mapping\n",
    "    ordr = df_map[[\n",
    "        \"ID\", \"rm_id\", \"forecast_start_date\", \"forecast_end_date\", \"cutoff_date\"\n",
    "    ]].merge(orders, on=\"rm_id\", how=\"left\")\n",
    "    \n",
    "    # Filter: only orders known before cutoff\n",
    "    known_mask = True\n",
    "    if \"created_date_time\" in ordr.columns:\n",
    "        known_mask = ordr[\"created_date_time\"] <= ordr[\"cutoff_date\"]\n",
    "    if \"modified_date_time\" in ordr.columns:\n",
    "        known_mask &= (\n",
    "            ordr[\"modified_date_time\"].isna() |\n",
    "            (ordr[\"modified_date_time\"] <= ordr[\"cutoff_date\"])\n",
    "        )\n",
    "    ordr = ordr[known_mask]\n",
    "    \n",
    "    # Filter: delivery expected in forecast window\n",
    "    in_window = (\n",
    "        (ordr[\"delivery_date\"] >= ordr[\"forecast_start_date\"]) &\n",
    "        (ordr[\"delivery_date\"] <= ordr[\"forecast_end_date\"])\n",
    "    )\n",
    "    \n",
    "    # Aggregate by ID\n",
    "    if {\"purchase_order_id\", \"purchase_order_item_no\"}.issubset(ordr.columns):\n",
    "        grp = ordr[in_window].groupby(\"ID\", as_index=False).agg(\n",
    "            orders_qty_in_window=(\"quantity\", \"sum\"),\n",
    "            orders_lines_in_window=(\"purchase_order_id\", \"nunique\")\n",
    "        )\n",
    "    else:\n",
    "        grp = ordr[in_window].groupby(\"ID\", as_index=False).agg(\n",
    "            orders_qty_in_window=(\"quantity\", \"sum\"),\n",
    "            orders_lines_in_window=(\"quantity\", \"size\")\n",
    "        )\n",
    "    \n",
    "    # Merge back\n",
    "    out = df_map.merge(grp, on=\"ID\", how=\"left\")\n",
    "    out[\"orders_qty_in_window\"] = out[\"orders_qty_in_window\"].fillna(0.0)\n",
    "    out[\"orders_lines_in_window\"] = out[\"orders_lines_in_window\"].fillna(0).astype(int)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def make_features_from_mapping(\n",
    "    daily: pd.DataFrame,\n",
    "    df_map_in: pd.DataFrame,\n",
    "    orders: pd.DataFrame | None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build feature matrix from prediction mapping.\n",
    "    \n",
    "    Features include:\n",
    "    - Historical rolling statistics (as of cutoff date)\n",
    "    - Same period last year comparison\n",
    "    - Calendar features\n",
    "    - Purchase order features (no data leakage)\n",
    "    \"\"\"\n",
    "    df_map = df_map_in.copy()\n",
    "    df_map[\"rm_id\"] = pd.to_numeric(df_map[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    \n",
    "    # Cutoff date is the day before forecast starts\n",
    "    df_map[\"cutoff_date\"] = df_map[\"forecast_start_date\"] - pd.Timedelta(days=1)\n",
    "    df_map[\"window_days\"] = (\n",
    "        df_map[\"forecast_end_date\"] - df_map[\"forecast_start_date\"]\n",
    "    ).dt.days + 1\n",
    "    \n",
    "    # Prepare historical features (rename date -> cutoff_date for join)\n",
    "    feat_date = daily.rename(columns={\"date\": \"cutoff_date\"}).copy()\n",
    "    feat_date[\"rm_id\"] = pd.to_numeric(\n",
    "        feat_date[\"rm_id\"], errors=\"coerce\"\n",
    "    ).astype(\"int64\")\n",
    "    feat_date = feat_date.sort_values([\"rm_id\", \"cutoff_date\"])\n",
    "    \n",
    "    hist_cols = [\n",
    "        \"rm_id\", \"cutoff_date\", \"cum_kg\",\n",
    "        \"r7_kg\", \"r14_kg\", \"r28_kg\", \"r56_kg\", \"r365_kg\",\n",
    "        \"r7_days\", \"r14_days\", \"r28_days\", \"r56_days\", \"r365_days\",\n",
    "        \"r28_mean_kg_per_day\", \"r56_mean_kg_per_day\", \"days_since_last\"\n",
    "    ]\n",
    "    \n",
    "    # Join historical features\n",
    "    X = df_map.merge(feat_date[hist_cols], on=[\"rm_id\", \"cutoff_date\"], how=\"left\")\n",
    "    \n",
    "    # Add same period last year\n",
    "    csum = daily[[\"rm_id\", \"date\", \"cum_kg\"]].copy()\n",
    "    X = X.merge(same_period_last_year(df_map, csum), on=\"ID\", how=\"left\")\n",
    "    \n",
    "    # Calendar features\n",
    "    X[\"start_month\"] = X[\"forecast_start_date\"].dt.month\n",
    "    X[\"start_dow\"] = X[\"forecast_start_date\"].dt.dayofweek\n",
    "    X[\"end_month\"] = X[\"forecast_end_date\"].dt.month\n",
    "    \n",
    "    # Purchase order features\n",
    "    X = add_orders_features(X, orders)\n",
    "    \n",
    "    # Fill missing values\n",
    "    numeric_cols = [\n",
    "        \"cum_kg\", \"r7_kg\", \"r14_kg\", \"r28_kg\", \"r56_kg\", \"r365_kg\",\n",
    "        \"r7_days\", \"r14_days\", \"r28_days\", \"r56_days\", \"r365_days\",\n",
    "        \"r28_mean_kg_per_day\", \"r56_mean_kg_per_day\", \"days_since_last\",\n",
    "        \"same_period_last_year_kg\", \"orders_qty_in_window\"\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].fillna(0.0)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def build_training_windows_template(\n",
    "    daily: pd.DataFrame,\n",
    "    mapping_like: pd.DataFrame,\n",
    "    step_days: int = 7,\n",
    "    min_history_days: int = 400\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic training windows from historical data.\n",
    "    \n",
    "    Creates multiple forecast windows for each rm_id by sliding through time.\n",
    "    Ensures sufficient history before each window starts.\n",
    "    \n",
    "    Args:\n",
    "        daily: Daily aggregated data\n",
    "        mapping_like: Example mapping to extract window lengths\n",
    "        step_days: Days between consecutive training windows\n",
    "        min_history_days: Minimum days of history required before first window\n",
    "    \"\"\"\n",
    "    # Extract unique window lengths from mapping\n",
    "    win_lengths = sorted(\n",
    "        (mapping_like[\"forecast_end_date\"] - mapping_like[\"forecast_start_date\"])\n",
    "        .dt.days.add(1)\n",
    "        .unique()\n",
    "    )\n",
    "    \n",
    "    pieces = []\n",
    "    \n",
    "    for rm, g in daily.groupby(\"rm_id\"):\n",
    "        first_date = g[\"date\"].min()\n",
    "        last_date = g[\"date\"].max()\n",
    "        \n",
    "        for wlen in win_lengths:\n",
    "            # First window starts after min_history_days\n",
    "            start = first_date + pd.Timedelta(days=min_history_days)\n",
    "            # Last window must fit completely in data\n",
    "            end_latest = last_date - pd.Timedelta(days=wlen)\n",
    "            \n",
    "            if start > end_latest:\n",
    "                continue\n",
    "            \n",
    "            # Generate sliding windows\n",
    "            starts = pd.date_range(start, end_latest, freq=f\"{step_days}D\")\n",
    "            df = pd.DataFrame({\n",
    "                \"rm_id\": rm,\n",
    "                \"forecast_start_date\": starts,\n",
    "                \"forecast_end_date\": starts + pd.Timedelta(days=wlen - 1)\n",
    "            })\n",
    "            pieces.append(df)\n",
    "    \n",
    "    if not pieces:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"ID\", \"rm_id\", \"forecast_start_date\", \"forecast_end_date\"]\n",
    "        )\n",
    "    \n",
    "    # Combine and assign IDs\n",
    "    tmpl = pd.concat(pieces, ignore_index=True)\n",
    "    tmpl = tmpl.sort_values([\"rm_id\", \"forecast_start_date\"]).reset_index(drop=True)\n",
    "    tmpl[\"ID\"] = np.arange(1, len(tmpl) + 1)\n",
    "    \n",
    "    return tmpl[[\"ID\", \"rm_id\", \"forecast_start_date\", \"forecast_end_date\"]]\n",
    "\n",
    "\n",
    "def label_from_daily(\n",
    "    daily: pd.DataFrame,\n",
    "    df_map: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate actual delivery totals for each forecast window (labels).\n",
    "    \n",
    "    Uses groupby approach to avoid merge_asof sorting issues.\n",
    "    \"\"\"\n",
    "    # Prepare daily data with cumulative sums\n",
    "    g = daily[[\"rm_id\", \"date\", \"net_weight\"]].copy()\n",
    "    g[\"rm_id\"] = pd.to_numeric(g[\"rm_id\"], errors=\"coerce\").astype(\"int64\")\n",
    "    g[\"date\"] = pd.to_datetime(g[\"date\"], utc=False, errors=\"coerce\")\n",
    "    g = g.dropna(subset=[\"rm_id\", \"date\"])\n",
    "    g = g.sort_values([\"rm_id\", \"date\"]).reset_index(drop=True)\n",
    "    g[\"cum\"] = g.groupby(\"rm_id\")[\"net_weight\"].cumsum()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each rm_id separately to avoid sorting issues\n",
    "    for rm_id, group in df_map.groupby(\"rm_id\"):\n",
    "        rm_daily = g[g[\"rm_id\"] == rm_id].copy()\n",
    "        \n",
    "        if rm_daily.empty:\n",
    "            # No data for this rm_id\n",
    "            for _, row in group.iterrows():\n",
    "                results.append({\n",
    "                    \"ID\": row[\"ID\"],\n",
    "                    \"y_window_kg\": 0.0\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        # For each forecast window\n",
    "        for _, row in group.iterrows():\n",
    "            start_date = row[\"forecast_start_date\"]\n",
    "            end_date = row[\"forecast_end_date\"]\n",
    "            \n",
    "            # Find cumulative sum at start (day before)\n",
    "            before_start = rm_daily[rm_daily[\"date\"] < start_date]\n",
    "            cum_a = before_start[\"cum\"].iloc[-1] if not before_start.empty else 0.0\n",
    "            \n",
    "            # Find cumulative sum at end\n",
    "            before_end = rm_daily[rm_daily[\"date\"] <= end_date]\n",
    "            cum_b = before_end[\"cum\"].iloc[-1] if not before_end.empty else 0.0\n",
    "            \n",
    "            # Label = difference\n",
    "            y_kg = max(0.0, cum_b - cum_a)\n",
    "            results.append({\n",
    "                \"ID\": row[\"ID\"],\n",
    "                \"y_window_kg\": y_kg\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FEATURE ENGINEERING PIPELINE\")\n",
    "    print(\"TDT4173 Course Project - Hydro ASA x Append Consulting\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Read input data\n",
    "    print(\"\\n[1/5] Reading input data...\")\n",
    "    df_map_raw = read_mapping(PREDICTION_MAPPING_CSV)\n",
    "    df_master = read_master(MASTER_DATA_CSV)\n",
    "    \n",
    "    print(f\"  Prediction mapping: {len(df_map_raw)} rows\")\n",
    "    print(f\"  Master receival data: {len(df_master)} rows\")\n",
    "    print(f\"  Unique raw materials: {df_master['rm_id'].nunique()}\")\n",
    "    \n",
    "    # Build daily features\n",
    "    print(\"\\n[2/5] Building daily aggregated features...\")\n",
    "    daily = build_daily(df_master)\n",
    "    print(f\"  Daily features: {len(daily)} rows\")\n",
    "    print(f\"  Date range: {daily['date'].min()} to {daily['date'].max()}\")\n",
    "    \n",
    "    # Read purchase orders\n",
    "    print(\"\\n[3/5] Reading purchase orders...\")\n",
    "    orders = read_orders_with_rm(PURCHASE_ORDERS_CSV, df_master)\n",
    "    if orders is not None:\n",
    "        print(f\" Purchase orders: {len(orders)} rows\")\n",
    "    else:\n",
    "        print(\" No purchase order data found\")\n",
    "    \n",
    "    # Generate submission features\n",
    "    print(\"\\n[4/5] Generating submission features...\")\n",
    "    X_submit = make_features_from_mapping(daily, df_map_raw, orders)\n",
    "    \n",
    "    out_cols = [\n",
    "        \"ID\", \"rm_id\", \"forecast_start_date\", \"forecast_end_date\",\n",
    "        \"cutoff_date\", \"window_days\",\n",
    "        \"cum_kg\", \"r7_kg\", \"r14_kg\", \"r28_kg\", \"r56_kg\", \"r365_kg\",\n",
    "        \"r7_days\", \"r14_days\", \"r28_days\", \"r56_days\", \"r365_days\",\n",
    "        \"r28_mean_kg_per_day\", \"r56_mean_kg_per_day\", \"days_since_last\",\n",
    "        \"same_period_last_year_kg\",\n",
    "        \"orders_qty_in_window\", \"orders_lines_in_window\",\n",
    "        \"start_month\", \"start_dow\", \"end_month\"\n",
    "    ]\n",
    "    \n",
    "    X_submit[out_cols].to_csv(FEATURES_SUBMISSION, index=False)\n",
    "    print(f\"  ✓ Wrote {len(X_submit)} rows -> {FEATURES_SUBMISSION}\")\n",
    "    \n",
    "    # Generate training features\n",
    "    print(\"\\n[5/5] Generating training features and labels...\")\n",
    "    try:\n",
    "        train_template = build_training_windows_template(\n",
    "            daily, df_map_raw,\n",
    "            step_days=28,\n",
    "            min_history_days=500\n",
    "        )\n",
    "        train_template = train_template.sample(n=100000, random_state=42)\n",
    "        print(f\" Training windows: {len(train_template)} rows\")\n",
    "        \n",
    "        X_train = make_features_from_mapping(daily, train_template, orders)\n",
    "        y_train = label_from_daily(daily, train_template)\n",
    "        \n",
    "        train = X_train.merge(y_train, on=\"ID\", how=\"left\")\n",
    "        train[out_cols + [\"y_window_kg\"]].to_csv(FEATURES_TRAINING, index=False)\n",
    "        print(f\" Wrote {len(train)} rows -> {FEATURES_TRAINING}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Training set build failed: {e}\")\n",
    "        print(\"    (This is OK if you only need submission features)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "    print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4173-course-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
