{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5fd77a9",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "This notebook is used for both training models and using them to generate predictions. Feel free to begin configuring model code!\n",
    "\n",
    "Below are some models we may use. Included are some observations concerning each of them.\n",
    "1. N-BEATS\n",
    "    - Sequence modelling.\n",
    "    - Easily interpretable\n",
    "    - Very performant for time-series forecasting.\n",
    "    - Far more stable than other RNN-based neural networks.\n",
    "    - [Docs](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.nbeats.html).\n",
    "2. XGBoost (or LightGBM)\n",
    "    - Feature modelling. Can use [TSfresh](https://tsfresh.readthedocs.io/en/latest/) or [Ts2Vec](https://github.com/WenjieDu/PyPOTS?tab=readme-ov-file#user-content-fn-48-143bda604d5e3bfec7be057bccbe8255) (considered better than TSfresh) to automatically generate and filter a lot of good features.\n",
    "    - Quick training\n",
    "    - Handles label-encoding of categoricals\n",
    "    - Performant with limited data\n",
    "    - Handles missing values\n",
    "    - Requires good-quality training data \n",
    "    - Got a tip that XGBoost can be configured to use quantile regression as the objective function. We need to use something like ```reg:quantileerror``` with the parameter ```alpha=0.2```. This way we optimize the model for the 20-percent quantile, such as the task asks for.\n",
    "    - [Docs](https://xgboost.readthedocs.io/en/stable/).\n",
    "\n",
    "3. DLinear\n",
    "    - Incredibly simple, yet surprisingly performant for seasonal and cyclical time-series.\n",
    "    - Essentially N-BEATS without neural networks.\n",
    "    - [Docs](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.dlinear.html).\n",
    "4. Temporal Fusion Transformer\n",
    "    - Sequence modelling. Handles time-series very well.\n",
    "    - Good interpretability.\n",
    "    - Requires a good deal of training data to perform well. Maybe we have enough, maybe not.\n",
    "    - [Docs](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.tft_model.html)\n",
    "5. Random Forest Classifier\n",
    "    - A simpler, less performant version of XGBoost.\n",
    "    - Feature modelling.\n",
    "    - [Docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda67a7c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0392ba0",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae5c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b32e45",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, folder=\"1_raw\"):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file in a subfolder of the project's 'data' directory.\n",
    "    This version is adjusted to work even if the notebook is run from a subfolder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The name of the file to load, including the extension (e.g., \"data.csv\").\n",
    "    folder : str, optional\n",
    "        The subfolder within 'data' to load from. Defaults to \"1_raw\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Go up one level from the current working directory to find the project root\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "        file_path = PROJECT_ROOT / \"data\" / folder / filename\n",
    "\n",
    "        df = pd.read_csv(file_path, sep=\",\")\n",
    "\n",
    "        print(f\"Data loaded successfully from {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_data(df, filename, folder=\"2_interim\"):\n",
    "    \"\"\"\n",
    "    Save a dataframe to a CSV file in a subfolder of the project's 'data' directory.\n",
    "\n",
    "    This function automatically creates the destination folder if it doesn't exist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to save.\n",
    "    filename : str\n",
    "        The name for the output file, including the extension (e.g., \"processed_orders.csv\").\n",
    "    folder : str, optional\n",
    "        The subfolder within 'data' to save to. Defaults to \"2_interim\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "        save_dir = PROJECT_ROOT / \"data\" / folder\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # The full filename, including extension, is now expected\n",
    "        file_path = save_dir / filename\n",
    "\n",
    "        df.to_csv(file_path, sep=\",\", index=False)\n",
    "\n",
    "        print(f\"Data saved successfully to {file_path} âœ…\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a83402",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748ee2a9",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "\n",
    "Setup (Blocks 1-4): You import libraries, load your data, and define your helper functions. Crucially, you now have a quantile_error_scorer that directly measures what the competition asks for.\n",
    "\n",
    "Optuna objective (Block 5): This is the heart of the tuning process. For each trial (a set of hyperparameters), it runs a full time-series cross-validation. It dynamically creates the correct training/validation sets for each fold, trains a model, and calculates the score. It returns the average score across the folds, which gives Optuna a stable target to minimize.\n",
    "\n",
    "Running the Study (Block 6): This block kicks off the optimization. It will run the objective function 50 times, intelligently searching for the best combination of hyperparameters.\n",
    "\n",
    "Final Model Training (Block 7): This is the critical final step. Once Optuna has found the best settings, you don't want to use one of the models from the cross-validation folds. You want a single, final model that has learned from all available historical data. This block trains that definitive model using the optimal parameters, ready for you to generate your final predictions for the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Block 1: Imports ---\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# --- Block 2: Data Loading & Initial Preparation ---\n",
    "# Load your raw dataframes here\n",
    "# datasets = load_all_your_csvs()\n",
    "# master_df = ... your full, merged historical dataframe ...\n",
    "# master_df['date_arrival'] = pd.to_datetime(master_df['date_arrival'])\n",
    "master_df = None\n",
    "\n",
    "# Create the full date range for TimeSeriesSplit\n",
    "# This should be based on your aggregated daily data\n",
    "# all_dates = pd.to_datetime(master_df['date_arrival']).dt.date.unique()\n",
    "# all_dates.sort()\n",
    "all_dates = []\n",
    "\n",
    "\n",
    "# --- Block 3: Feature and Target Creation Functions ---\n",
    "# It's best practice to put your feature engineering logic into functions.\n",
    "\n",
    "\n",
    "def create_features(data, last_hist_date):\n",
    "    \"\"\"\n",
    "    Creates features for all rm_ids based on historical data up to a cutoff date.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The master dataframe with all historical data.\n",
    "        last_hist_date (pd.Timestamp): The last date to include for feature calculation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe where each row is an rm_id and each column is a feature.\n",
    "    \"\"\"\n",
    "    # This is where your feature engineering logic goes.\n",
    "    # For now, this is a placeholder.\n",
    "    # Example:\n",
    "    # features = data[data['date_arrival'] <= last_hist_date].groupby('rm_id').agg(\n",
    "    #     avg_net_weight=('net_weight', 'mean'),\n",
    "    #     std_net_weight=('net_weight', 'std')\n",
    "    # )\n",
    "    # return features\n",
    "    pass  # Replace with your actual feature engineering\n",
    "\n",
    "\n",
    "def create_target(data, forecast_start_date, forecast_end_date):\n",
    "    \"\"\"\n",
    "    Creates the cumulative target variable for a given forecast period.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The master dataframe.\n",
    "        forecast_start_date (pd.Timestamp): The first day of the forecast period.\n",
    "        forecast_end_date (pd.Timestamp): The last day of the forecast period.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A series where the index is rm_id and the value is the cumulative net_weight.\n",
    "    \"\"\"\n",
    "    # This is where your target creation logic goes.\n",
    "    # For now, this is a placeholder.\n",
    "    # Example:\n",
    "    # target_period = data[\n",
    "    #     (data['date_arrival'] >= forecast_start_date) &\n",
    "    #     (data['date_arrival'] <= forecast_end_date)\n",
    "    # ]\n",
    "    # target = target_period.groupby('rm_id')['net_weight'].sum()\n",
    "    # return target\n",
    "    pass  # Replace with your actual target creation\n",
    "\n",
    "\n",
    "# --- Block 4: Custom Scorer for the Competition Metric ---\n",
    "def quantile_error_scorer(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom scoring function for the Quantile Error at alpha=0.2.\n",
    "    \"\"\"\n",
    "    alpha = 0.2\n",
    "    error = y_true - y_pred\n",
    "    loss = np.maximum(alpha * error, (alpha - 1) * error)\n",
    "    return np.mean(loss)\n",
    "\n",
    "\n",
    "# --- Block 5: Optuna Objective Function ---\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    The function Optuna will minimize. It trains a model using TimeSeriesSplit\n",
    "    and returns the average validation score.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"objective\": \"quantile\",\n",
    "        \"alpha\": 0.2,\n",
    "        \"metric\": \"quantile\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 2000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 300),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)  # Use 3-5 splits for tuning\n",
    "    scores = []\n",
    "\n",
    "    # This loop is for robust validation of a single hyperparameter set\n",
    "    for train_indices, val_indices in tscv.split(all_dates):\n",
    "        # Determine the time period for this fold\n",
    "        train_cutoff_date = all_dates[train_indices[-1]]\n",
    "\n",
    "        # NOTE: This assumes a fixed forecast horizon for validation, e.g., 30 days\n",
    "        validation_start_date = all_dates[val_indices[0]]\n",
    "        validation_end_date = all_dates[val_indices[-1]]\n",
    "\n",
    "        # Create datasets for this specific fold\n",
    "        X_train = create_features(master_df, train_cutoff_date)\n",
    "        y_train = create_target(master_df, validation_start_date, validation_end_date)\n",
    "        X_val = X_train  # Features are the same, as they are based on past data\n",
    "        y_val = y_train\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False)],\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        score = quantile_error_scorer(y_val, preds)\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# --- Block 6: Running the Optuna Study ---\n",
    "print(\"Starting hyperparameter tuning with Optuna...\")\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)  # Start with 50 trials, increase if needed\n",
    "\n",
    "# --- Block 7: Getting Results and Training the Final Model ---\n",
    "print(\"\\n--- OPTUNA RESULTS ---\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"  Value (Quantile Error): {best_trial.value:.4f}\")\n",
    "print(\"  Best Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = best_trial.params\n",
    "# Ensure objective and alpha are correctly set for the final model\n",
    "best_params[\"objective\"] = \"quantile\"\n",
    "best_params[\"alpha\"] = 0.2\n",
    "\n",
    "print(\"\\n--- Training Final Model on All Historical Data ---\")\n",
    "# Now, train one final model on ALL your historical data using the best params\n",
    "# The features are based on all data up to the start of the forecast period\n",
    "final_X_train = create_features(master_df, pd.to_datetime(\"2024-12-31\"))\n",
    "# The target is what you want to predict in the future, so you don't have a y_train here.\n",
    "# For the final fit, we use the model to learn from all data without a validation set.\n",
    "\n",
    "final_model = lgb.LGBMRegressor(**best_params)\n",
    "\n",
    "# We fit on all available feature data. There is no y_train because the \"target\" is in the future.\n",
    "# The model learns the patterns from the features of all rm_ids up to the end of 2024.\n",
    "# This is a common approach in forecasting competitions.\n",
    "final_model.fit(\n",
    "    final_X_train, y=None\n",
    ")  # y is None as we are just training on the final feature set\n",
    "\n",
    "print(\"Final model trained successfully!\")\n",
    "# You would now use 'final_model' to predict on your 'X_test' set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717d3d9",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8c761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4efad92e",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4173-course-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
